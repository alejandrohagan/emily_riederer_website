{
  "hash": "554103dfbbf9fa1b34f99de7bd384215",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Base Python Rgonomic Patterns\"\ndescription: \"Getting comfortable in a new language is more than the packages you use. Syntactic sugar in base python increases the efficiency, and aesthetics of python code in ways that R users may enjoy in packages like `glue` and `purrr`. This post collects a miscellaneous grab bag of tools for wrangling, formatting (f-strings), repeating (list comprehensions), faking data, and saving objects (pickle)\"\nauthor: \"Emily Riederer\"\ndate: \"2024-01-20\"\ncategories: [rstats, python, tutorial]\nimage: \"featured.jpg\"\n---\n\n![ Photo credit to [David Clode](https://unsplash.com/@davidclode) on Unsplash ](featured.jpg)\n\nIn the past few weeks, I've been writing about a [stack of tools](/post/py-rgo) and [specific packages like `polars`](/post/py-rgo-polars/) that may help R users feel \"at home\" when working in python due to similiar ergonomics. However, one common snag in switching languages is ramping up on common \"recipes\" for higher-level workflows (e.g. how to build a `sklearn` modeling pipeline) but missing a languages's fundamentals that make writing glue code feel smooth (and dare I say pleasant?) It's a maddening feeling to get code for a *complex* task to finish only to have the result wrapped in an object that you can't suss out how to save or manipulate.\n\nThis post goes back to the basics. We'll briefly reflect on a few aspects of usability that have led to the success of many workflow packages in R. Then, I'll demonstrate a grab bag of coding patterns in python that make it feel more elegant to connect bits of code into a coherent workflow. \n\nWe'll look at the kind of functionality that you didn't know to miss until it was gone, you may not be quite sure what to search to figure out how to get it back, *and* you wonder if it's even reasonable to hope there's an analog^[I defined this odd scope to help limit the infinite number of workflow topics that could be included like \"how to write a function\" or \"how to source code from another script\"]. This won't be anything groundbreaking -- just some nuts and bolts. Specifically: helper functions for data and time manipulation, advanced string interpolation, list comprehensions for more functional programming, and object serialization. \n\n## What other R ergonomics do we enjoy?\n\nR's passionate user and developer community has invested a lot in building tools that smooth over rough edges and provide slick, concise APIs to rote tasks. Sepcifically, a number of packages are devoted to:\n\n- **Utility functions**: Things that make it easier to \"automate the boring stuff\" like `fs` for naviating file systems or `lubridate` for more semantic date wrangling\n- **Formatting functions**: Things that help us make things look nice for users like `cli` and `glue` to improve human readability of terminal output and string interpolation\n- **Efficiency functions**: Things that help us write efficient workflows like `purrr` which provides a concise, typesafe interface for iteration\n\nAll of these capabilities are things we *could* somewhat trivially write ourselves, but we don't *want* to and we don't *need* to. Fortunately, we don't need to in python either.\n\n## Wrangling Things (Date Manipulation)\n\nI don't know a data person who loves dates. In the R world, many enjoy `lubridate`'s wide range of helper functions for cleaning, formatting, and computing on dates. \n\nPython's `datetime` module is similarly effective. We can easily create and manage dates in `date` or `datetime` classes which make them easy to work with.\n\n::: {#5fab6f77 .cell execution_count=1}\n``` {.python .cell-code}\nimport datetime\nfrom datetime import date\ntoday = date.today()\nprint(today)\ntype(today)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2024-01-20\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\ndatetime.date\n```\n:::\n:::\n\n\nTwo of the most important functions are `strftime()` and `strptime()`.\n\n`strftime()` *formats* dates into strings. It accepts both a date and the desired string format. Below, we demonstrate by commiting the cardinal sin of writing a date in non-ISO8601. \n\n::: {#44894a15 .cell execution_count=2}\n``` {.python .cell-code}\ntoday_str = datetime.datetime.strftime(today, '%m/%d/%Y')\nprint(today_str)\ntype(today_str)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n01/20/2024\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nstr\n```\n:::\n:::\n\n\n`strptime()` does the opposite and turns a string encoding a date into an actual date. It can try to guess the format, or we can be nice and provide it guidance.\n\n::: {#311c5e5a .cell execution_count=3}\n``` {.python .cell-code}\nsomeday_dtm = datetime.datetime.strptime('2023-01-01', '%Y-%m-%d')\nprint(someday_dtm)\ntype(someday_dtm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2023-01-01 00:00:00\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\ndatetime.datetime\n```\n:::\n:::\n\n\nDate math is also relatively easy with `datetime`. For example, you can see we calculate the date difference simply by... taking the difference! From the resulting delta object, we can access the `days` attribute.\n\n::: {#e2dada67 .cell execution_count=4}\n``` {.python .cell-code}\nn_days_diff = ( today - someday_dtm.date() )\nprint(n_days_diff)\ntype(n_days_diff)\ntype(n_days_diff.days)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n384 days, 0:00:00\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nint\n```\n:::\n:::\n\n\n## Formatting Things (f-strings)\n\nR's `glue` is beloved for it's ability to easily combine variables and texts into complex strings without a lot of ugly, nested `paste()` functions.\n\npython has a number of ways of doing this, but the most readable is the newest: f-strings. Simply put an `f` before the string and put any variable names to be interpolated in `{`curly braces`}`.\n\n::: {#e9a3ac38 .cell execution_count=5}\n``` {.python .cell-code}\nname = \"Emily\"\nprint(f\"This blog post is written by {name}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThis blog post is written by Emily\n```\n:::\n:::\n\n\nf-strings also support formatting with formats specified after a colon. Below, we format a long float to round to 2 digits. \n\n::: {#cd36dc7a .cell execution_count=6}\n``` {.python .cell-code}\nproportion = 0.123456789\nprint(f\"The proportion is {proportion:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe proportion is 0.12\n```\n:::\n:::\n\n\nAny python expression -- not just a single variable -- can go in curly braces. So, we can instead format that propotion as a percent. \n\n::: {#00b368b6 .cell execution_count=7}\n``` {.python .cell-code}\nproportion = 0.123456789\nprint(f\"The proportion is {proportion*100:.1f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe proportion is 12.3%\n```\n:::\n:::\n\n\nDespite the slickness of f-strings, sometimes other string interpolation approaches can be useful. For example, if all the variables I want to interpolate are in a dictionary (as often will happen, for example, with REST API responses), the string `format()` method is a nice alternative. It allows us to pass in the dictionary, \"unpacking\" the argument with `**`^[This is called \"**kwargs\" and works a bit like `do.call()` in base R. You can read more about it [here](https://www.digitalocean.com/community/tutorials/how-to-use-args-and-kwargs-in-python-3).]\n\n::: {#edc01047 .cell execution_count=8}\n``` {.python .cell-code}\nresult = {\n    'dog_name': 'Squeak',\n    'dog_type': 'Chihuahua'\n}\nprint(\"{dog_name} is a {dog_type}\".format(**result))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSqueak is a Chihuahua\n```\n:::\n:::\n\n\n### Application: Generating File Names\n\nCombining what we've discussed about `datetime` and f-strings, here's a pattern I use frequently. If I am logging results from a run of some script, I might save the results in a file suffixed with the run timestamp. We can generate this easily.\n\n::: {#a52f392c .cell execution_count=9}\n``` {.python .cell-code}\ndt_stub = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\nfile_name = f\"output-{dt_stub}.csv\"\nprint(file_name)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\noutput-20240120_061858.csv\n```\n:::\n:::\n\n\n## Repeating Things (Iteration / Functional Programming)\n\nThanks in part to a modern-day fiction that `for` loops in R are inefficient, R users have gravitated towards concise mapping functions for iteration. These can include the `*apply()` family^[Speaking of non-ergonomic things in R, the `*apply()` family is notoriously diverse in its number and order of arguments], `purrr`'s `map_*()` functions, or the parallelized version of either. \n\nPython too has a nice pattern for arbitrary iteration in list comprehensions. For any iterable, we can use a list comprehension to make a list of outputs by processing a list of inputs, with optional conditional and default expressions.\n\nHere are some trivial examples:\n\n::: {#e7885c25 .cell execution_count=10}\n``` {.python .cell-code}\nl = [1,2,3]\n[i+1 for i in l]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n[2, 3, 4]\n```\n:::\n:::\n\n\n::: {#3a8923b9 .cell execution_count=11}\n``` {.python .cell-code}\n[i+1 for i in l if i % 2 == 1]\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n[2, 4]\n```\n:::\n:::\n\n\n::: {#6307d9a3 .cell execution_count=12}\n``` {.python .cell-code}\n[i+1 if i % 2 == 1 else i for i in l]\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n[2, 2, 4]\n```\n:::\n:::\n\n\nThere are also closer analogs to `purrr` like python's `map()` function. `map()` takes a function and an iterable object and applies the function to each element. Like with `purrr`, functions can be anonymous (as defined in python with lambda functions) or named. List comprehensions are popular for their concise syntax, but there are many different thoughts on the matter as expressed in [this StackOverflow post](https://stackoverflow.com/questions/1247486/list-comprehension-vs-map). \n\n::: {#227dd77a .cell execution_count=13}\n``` {.python .cell-code}\ndef add_one(i): \n  return i+1\n\n# these are the same\nlist(map(lambda i: i+1, l))\nlist(map(add_one, l))\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n[2, 3, 4]\n```\n:::\n:::\n\n\n### Application: Simulation\n\nAs a (slightly) more realistic(ish) example, let's consider how list comprehensions might help us conduct a numerical simulation or sensitivity analysis. \n\nSuppose we want to simulate 100 draws from a Bernoulli distribution with different success probabilites and see how close our empirically calculated rate is to the true rate.\n\nWe can define the probabilites we want to simulate in a list and use a list comprehension to run the simulations.\n\n::: {#5cf16897 .cell execution_count=14}\n``` {.python .cell-code}\nimport numpy as np\nimport numpy.random as rnd\n\nprobs = [0.1, 0.25, 0.5, 0.75, 0.9]\ncoin_flips = [ np.mean(np.random.binomial(1, p, 100)) for p in probs ]\ncoin_flips\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n[0.13, 0.18, 0.52, 0.75, 0.87]\n```\n:::\n:::\n\n\nAlternatively, instead of returning a list of the same length, our resulting list could include whatever we want -- like a list of lists! If we wanted to keep the raw simulation results, we could. The following code returns a list of 5 lists - one with the raw simulation results.\n\n::: {#de2b5e6a .cell execution_count=15}\n``` {.python .cell-code}\ncoin_flips = [ list(np.random.binomial(1, p, 100)) for p in probs ]\nprint(f\"\"\"\n  coin_flips has {len(coin_flips)} elements\n  Each element is itself a {type(coin_flips[0])}\n  Each element is of length {len(coin_flips[0])}\n  \"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  coin_flips has 5 elements\n  Each element is itself a <class 'list'>\n  Each element is of length 100\n  \n```\n:::\n:::\n\n\nIf one wished, they could then put these into a `polars` dataframe and pivot those list-of-lists (going from a 5-row dataset to a 500-row dataset)to conduct whatever sort of analysis with want with all the replicates.\n\n::: {#fff0d519 .cell execution_count=16}\n``` {.python .cell-code}\nimport polars as pl\n\ndf_flips = pl.DataFrame({'prob': probs, 'flip': coin_flips})\ndf_flips.explode('flip').glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 500\nColumns: 2\n$ prob <f64> 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1\n$ flip <i32> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0\n\n```\n:::\n:::\n\n\n## Faking Things (Data Generation)\n\nCreating simple miniature datasets is often useful in analysis. When working with a new packages, it's an important part of learning, developing, debugging, and eventually unit testing. We can easily run our code on a simplified data object where the desired outcome is easy to determine to sanity-check our work, or we can use fake data to confirm our understanding of how a program will handle edge cases (like the diversity of ways different programs [handle null values](/post/nulls-polyglot/)). \n\nIn R, `data.frame()` and `expand.grid()` are go-to functions, coupled with vector generators like `rep()` and `seq()`. Python has many similar options.\n\n### Fake Datasets\n\nFor the simplest of datasets, we can manually write a few entries as with `data.frame()` in R. Here, we define series in a named dictionary where each dictionary key turns into a column name.\n\n::: {#af5ce75b .cell execution_count=17}\n``` {.python .cell-code}\nimport polars as pl\n\npl.DataFrame({\n  'a': [1,2,3],\n  'b': ['x','y','z']\n})\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th></tr><tr><td>i64</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;x&quot;</td></tr><tr><td>2</td><td>&quot;y&quot;</td></tr><tr><td>3</td><td>&quot;z&quot;</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nIf we need longer datasets, we can use helper functions in packages like `numpy` to generate the series. Methods like `arange` and `linspace` work similarly to R's `seq()`. \n\n::: {#f2596596 .cell execution_count=18}\n``` {.python .cell-code}\nimport polars as pl\nimport numpy as np\n\npl.DataFrame({\n  'a': np.arange(stop = 3),\n  'b': np.linspace(start = 9, stop = 24, num = 3)\n})\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th></tr><tr><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>9.0</td></tr><tr><td>1</td><td>16.5</td></tr><tr><td>2</td><td>24.0</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nIf we need groups in our sample data, we can use `np.repeat()` which works like R's `rep(each = TRUE)`. \n\n::: {#4cefdf84 .cell execution_count=19}\n``` {.python .cell-code}\npl.DataFrame({\n  'a': np.repeat(np.arange(stop = 3), 2),\n  'b': np.linspace(start = 3, stop = 27, num = 6)\n})\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (6, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th></tr><tr><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>3.0</td></tr><tr><td>0</td><td>7.8</td></tr><tr><td>1</td><td>12.6</td></tr><tr><td>1</td><td>17.4</td></tr><tr><td>2</td><td>22.2</td></tr><tr><td>2</td><td>27.0</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nAlternatively, for more control and succinct typing, we can created a nested dataset in `polars` and explode it out.\n\n::: {#fd81db4c .cell execution_count=20}\n``` {.python .cell-code}\n(\n  pl.DataFrame({\n    'a': [1, 2, 3],\n    'b': [\"a b c\", \"d e f\", \"g h i\"]\n  })\n  .with_columns(pl.col('b').str.split(\" \"))\n  .explode('b')\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th></tr><tr><td>i64</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;a&quot;</td></tr><tr><td>1</td><td>&quot;b&quot;</td></tr><tr><td>1</td><td>&quot;c&quot;</td></tr><tr><td>2</td><td>&quot;d&quot;</td></tr><tr><td>2</td><td>&quot;e&quot;</td></tr><tr><td>2</td><td>&quot;f&quot;</td></tr><tr><td>3</td><td>&quot;g&quot;</td></tr><tr><td>3</td><td>&quot;h&quot;</td></tr><tr><td>3</td><td>&quot;i&quot;</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nSimilarly, we could use what we've learned about list columns.\n\n::: {#b426a448 .cell execution_count=21}\n``` {.python .cell-code}\na = [1, 2, 3]\nb = [ [q*i for q in [1, 2, 3]] for i in a]\npl.DataFrame({'a':a,'b':b}).explode('b')\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th></tr><tr><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>1</td></tr><tr><td>1</td><td>2</td></tr><tr><td>1</td><td>3</td></tr><tr><td>2</td><td>2</td></tr><tr><td>2</td><td>4</td></tr><tr><td>2</td><td>6</td></tr><tr><td>3</td><td>3</td></tr><tr><td>3</td><td>6</td></tr><tr><td>3</td><td>9</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### Built-In Data\n\nR has a number of canonical datasets like `iris` built in to the core language. This can be easy to quickly grab for experimentation^[Particularly if you want to set wildly unrealistic expectations for the efficacy of k-means clustering, but I digress]. While base python doesn't include such capabilities, many of the exact same or similar datasets can be found in `seaborn`. \n\n`seaborn.get_dataset_names()` provides the list of available options. Below, we load the Palmers Penguins data and, if you wish, convert it from `pandas` to `polars`.\n\n::: {#ddbf65f8 .cell execution_count=22}\n``` {.python .cell-code}\nimport seaborn as sns\nimport polars as pl\n\ndf_pd = sns.load_dataset('penguins')\ndf = pl.from_pandas(df_pd)\ndf.glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 344\nColumns: 7\n$ species           <str> 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie'\n$ island            <str> 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen'\n$ bill_length_mm    <f64> 39.1, 39.5, 40.3, None, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0\n$ bill_depth_mm     <f64> 18.7, 17.4, 18.0, None, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2\n$ flipper_length_mm <f64> 181.0, 186.0, 195.0, None, 193.0, 190.0, 181.0, 195.0, 193.0, 190.0\n$ body_mass_g       <f64> 3750.0, 3800.0, 3250.0, None, 3450.0, 3650.0, 3625.0, 4675.0, 3475.0, 4250.0\n$ sex               <str> 'Male', 'Female', 'Female', None, 'Female', 'Male', 'Female', 'Male', None, None\n\n```\n:::\n:::\n\n\n## Saving Things (Object Serialization)\n\nSometimes, it can be useful to save *objects* as they existed in RAM in an active programming environment. R users may have experienced this if they've used `.rds`, `.rda`, or `.Rdata` files to save individual variables or their entire environment. These objects can often be faster to reload than plaintext and can better preserve information that may be lost in other formats (e.g. storing a dataframe in a way that preserves its datatypes versus writing to a CSV file^[And yes, you can and should use Parquet and then my example falls apart -- but that's not the point!] or storing a complex object that can't be easily reduced to plaintext like a model with training data, hyperparameters, learned tree splits or weights or whatnot for future predictions.) This is called object serializaton^[And, if you want to go incredibly deep here, check out [this awesome post](https://blog.djnavarro.net/posts/2021-11-15_serialisation-with-rds/) by Danielle Navarro.]\n\nPython has comparable capabilities in the [`pickle` module](https://docs.python.org/3/library/pickle.html). There aren't really style points here, so I've not much to add beyond \"this exists\" and \"read the documentation\". But, at a high level, it looks something like this:\n\n::: {#41b91385 .cell execution_count=23}\n``` {.python .cell-code}\n# to write a pickle\nwith open('my-obj.pickle', 'wb') as handle:\n    pickle.dump(my_object, handle, protocol = pickle.HIGHEST_PROTOCOL)\n\n# to read a pickle\nmy_object = pickle.load(open('my-obj.pickle','rb'))\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}