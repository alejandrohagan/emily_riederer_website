{
  "hash": "c2c4451fb66ba924ca4d835d02ec82df",
  "result": {
    "markdown": "---\ntitle: \"Update: grouped data quality check PR merged to dbt-utils\"\ndescription: \"After a prior post on the merits of grouped data quality checks, I demo my newly merged implementation for dbt\"\nauthor: \"Emily Riederer\"\ndate: \"2022-08-26\"\ncategories: [data, dbt]\nimage: \"featured.jpg\"\n---\n\n![ Photo credit to [Greyson Joralemon](https://unsplash.com/@greysonjoralemon) on Unsplash ](featured.jpg)\n\nLast fall, I wrote about the [unreasonably effectiveness of grouping in data quality checks](https://www.emilyriederer.com/post/grouping-data-quality/). In this follow-up, I want to share that my [pull request](https://github.com/dbt-labs/dbt-utils/pull/633) for such features has just been merged into the development branch of the `dbt-utils` package, a common add-on to the `dbt` data transformation stack. This feature will officially \"go live\" in the 1.0.0 version release that is planned for later this fall. \n\nIn this brief post, I'll recall the benefits of such checks (which my original post further illustrates with NYC subway data) and demonstrate how these checks can now be implemented in `dbt-utils`. \n\nFor those interested, I'll also provide a brief overview of how I implemented this change, but I recommend checking out the PR itself for complete details.\n\n## Recap\n\nTo recap the benefits of such checks from my initial post:\n\n- Some data checks can only be expressed within a group (e.g. ID values should be unique within a group but can be repeated between groups)\n- Some data checks are more precise when done by group (e.g. not only should table row-counts be equal but the counts within each group should be equal)\n\nOf course, these benefits are more or less relevant to different types of data checks. My PR updates the following tests:\n\n- equal_rowcount()\n- recency()\n- fewer_rows_than()\n- at_least_one()\n- not_constant()\n- non_null_proportion()\n- sequential_values()\n\nOf these checks, most fall in the category of providing more rigor when being conducted at the group level. Only the `sequential_values()` test is often unable to be expressed without grouping.\n\n## Demo\n\n[Data tests](https://docs.getdbt.com/docs/building-a-dbt-project/tests) in `dbt` are specified in the `schema.yml` file for relevant models. Adding grouping to the tests listed above will now be as simple as adding a `group_by_columns` key-value pair to the tests, as desired, which accepts either a single variable name or a list of variables to be used for grouping.\n\n\n```{yaml eval = FALSE}\n  - name: data_test_at_least_one\n    columns:\n      - name: field\n        tests:\n          - dbt_utils.at_least_one:\n              group_by_columns: ['grouping_column']\n```\n\n\nFor those that have not used `dbt`'s data testing framework before, this configuration is then used to generate SQL (now with the custom `GROUP BY` clause) which are evaluated when `dbt test` is run.  \n\n## Implementation\n\nIn implementing this PR, I considered a few core principles:\n\n- Make this feature as unobtrusive and isolated as possible with respect to the macros broader implementation\n- Follow standard DRY principles (e.g. specifically, render needed text as few times as possible)\n- Implement consistently across macros\n\nWith these principles in mind, the majority of implementations are like that of the `recency` macro where all relevant SQL strings are pre-computed:\n\n```\n{% set threshold = dbt_utils.dateadd(datepart, interval * -1, dbt_utils.current_timestamp()) %}\n{% if group_by_columns|length() > 0 %}\n  {% set select_gb_cols = group_by_columns|join(' ,') + ', ' %}\n  {% set groupby_gb_cols = 'group by ' + group_by_columns|join(',') %}\n{% endif %}\n```\n\nThe main deviations to this were the sequential() macro (requiring a window function) and the equal_rowcount()/fewer_rows_than() (requiring joins)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}