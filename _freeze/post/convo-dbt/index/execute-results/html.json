{
  "hash": "7a4bea7da8a73a87c8ec0f8d889d2814",
  "result": {
    "markdown": "---\ntitle: \"Embedding column-name contracts in data pipelines with dbt\"\ndescription: \"dbt supercharges SQL with Jinja templating, macros, and testing -- all of which can be customized to enforce controlled vocabularies and their implied contracts on a data model\"\nauthor: \"Emily Riederer\"\ndate: \"2021-02-06\"\ncategories: [data, sql, dbt]\nimage: \"featured.png\"\naliases:\n  - /post/convo-dbt/\n---\n\n\n![Data model DAG autogenerated by dbt](featured.png)\nIn my post [Column Names as Contracts](/post/column-name-contracts/), I explore how using controlled vocabularies to name fields in a data table can create performance contracts between data producers and data consumers^[Note that, in many cases, the distinction between a data producer and consumer is transient and somewhat arbitrary. In many cases, the same person can be both. Here, I use the terms mostly to differentiate the *goal* of a specific step of work. By \"data producer\", I mean someone engaged in the act of wrangling source data into a form suitable for analysis; by \"data consumer\", I mean someone actually using that wrangled data for reporting, analysis, visualization, modeling, etc.]. In short, I argue that field names can encode metadata^[As one *example* -- not a prescription for how all such vocabularies should work -- one might define that all counts start with `N_` and are non-negative integers; all identified start with `ID_` and are non-null] and illustrate with R and python how these names can be used to improve data documentation, wrangling, and validation. \n\nHowever, demonstrations with R and python are biased towards the needs of data consumers. These popular data analysis tools provide handy, high-level interfaces for programmatically operating on columns. For example, `dplyr`'s [select helpers](https://tidyselect.r-lib.org/reference/select_helpers.html) make it easy to quickly manipulate all columns whose names match given patterns. For example, suppose I know that all variables beginning with `IND_` are binary and non-null so I may sum them to get a count or average them to get a valid proportion. I can succinctly write:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarize_at(my_data,\n             .vars = vars(starts_with(\"IND\")),\n             .funs = list(sum, mean))\n```\n:::\n\n\nIn contrast, SQL remains a mainstay for data producers -- both for use in traditional relational databases and SQL interfaces for modern large-scale data processing engines like Spark. As a *very* high-level and declarative language, SQL variants generally don't offer a control flow (e.g. for loops, if statements) or programmatic control which would allow for column operations that are similar to the one shown above. That is, one might have to manually write:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nselect \n  mean(ind_a), \n  mean(ind_b), \n  sum(ind_a), \n  sum(ind_b)\nfrom my_data\n```\n:::\n\n\nBut that is tedious, static (would not automatically adapt to the addition of more indicator variables), and error-prone (easy to miss or mistype a variable). \n\nAlthough SQL itself is relatively inflexible, recent tools have added a layer of \"programmability\" on top of SQL which affords far more flexibility and customization. In this post, I'll demonstrate how one such tool, `dbt`, can help data producers consistently apply controlled vocabularies when defining, manipulating, and testing tables for analytical users.\n\n*(In fact, after writing this post, I've also begun experimenting with a [dbt package, `dbt_dplyr`](https://github.com/emilyriederer/dbt_dplyr) that brings `dplyr`'s select-helper semantics to SQL.)*\n\n## A brief intro to `dbt`\n\n`dbt` ([Data Build Tool](https://www.getdbt.com/)) \"applies the principles of software engineering to analytics code\". Specifically, it encourages data producers to write modular, atomic SQL `SELECT` statements in separate files (as opposed to the use of CTEs or subqueries) from which dbt derives a DAG and orchestrates the execution on your database of choice^[`dbt` has adapters for most major databases and engines including Amazon Redshift, Snowflake, and Apache Spark. An up-to-date list is available [here](https://docs.getdbt.com/docs/available-adapters/)]. Further, it enables the ability to write more programmatic (with control flow) SQL *templates* with `Jinja2` which `dbt` compiles to standard SQL files before executing. \n\nFor the purposes of implementing a controlled vocabulary, key advantages of this approach include:\n\n- Templating with `if` statements and `for` loops\n- Dynamic insertion of local variables^[Some but not all databases natively support local variables, but `dbt`'s approach works equally well with those that do not]\n- Automated testing of each modular SQL unit \n- Code sharing with tests and macros exportable in a package framework\n\nAdditional slick (but tangential for this post) `dbt` features include:\n\n- The ability to switch between dev and production schemas\n- Easy toggling between views, tables, and inserts for the same base logic\n- Automatic generation of a static website documenting data lineage, metadata, and test results (the featured image above is a screenshot from the created website)\n- Orchestration of SQL statements in the DAG\n- Hooks for rote database management tasks like adding indices and keys or granting access\n\nFor a general overview to `dbt`, check out the [introductory tutorial](https://docs.getdbt.com/tutorial/setting-up) on their website, the [dbt101 presentation](https://www.getdbt.com/coalesce/agenda/dbt-101-eu-and-us-friendly) from their recent Coalesce conference^[One excellent feature of this project is the impressive amount of onboarding and documentation materials], or the interview with one of their founders on the [Data Engineering Today](https://open.spotify.com/episode/1gKKgR8eZgdqdXztFGGkFe) podcast.\n\nIn this post, I'll demonstrate how three features of `dbt` can support the use of controlled vocabulary column naming by:\n\n- Creating variable names that adhere to conventions with [Jinja templating](#variable-creation-with-jinja-templating)\n- Operating on subgroups of columns created by [custom macros](#variable-manipulation-with-regex-macros) to enforce contracts\n- Validating subgroups of columns to ensure adherence to contracts with [custom tests](#data-validation-with-custom-tests)\n\n## Scenario: COVID Forecast Model Monitoring\n\nThe full example code for this project is available [on GitHub](http://github.com/emilyriederer/dbt-convo-covid).\n\nTo illustrate these concepts, imagine we are tasked with monitoring the performance of a county-level COVID forecasting model using data similar to datasets available through [Google BigQuery public dataset program](https://cloud.google.com/blog/products/data-analytics/publicly-available-covid-19-data-for-analytics). We might want to continually log forecasted versus actual observations to ask questions like:\n\n- Does the forecast perform well?\n- How far in advance does the forecast become reliable?\n- How does performance vary across counties?\n- Is the performance acceptable in particularly sensitive counties, such as those with known health professional shortages?\n\nBefore we go further, a few caveats:\n\n- I am not a COVID expert nor do I pretend to be. This is not a post about how one should monitor a COVID model. This is just an understandable, hypothetical example with data in a publicly available database^[In fact, many COVID models were unduly criticized because their purpose was not strictly to have the most accurate forecast possible.] \n- I do not attempt to demonstrate the best way to evaluate a forecasting model or a holistic approach to model monitoring. Again, this is just a hypothetical motivation to illustrate *data management* techniques\n- This may seem like significant over-engineering for the problem at hand. Once again, this is just an example\n\nNow, back to work.\n\n### Controlled Vocabulary\n\nTo operationalize this analytical goal, we might start out by defining our controlled vocabulary with relevant concepts and contracts.\n\n**Units of measurement**:\n\n- `ID`: Unique identifier of entity with no other semantic meaning\n  + Non-null\n- `N`: Count\n  + Integer\n  + Non-null\n- `DT`: Date\n  + Date format\n- `IND`: Binary indicator\n  + Values of 0 or 1\n  + Non-null\n- `PROP`: Proportion\n  + Numeric\n  + Bounded between 0 and 1\n- `PCT`: Percent\n  + Numeric\n  + Unlike `PROP`, *not* bounded (e.g. think \"percent error\")\n- `CD`: System-generated character\n  + Non-null\n- `NM`: Human-readable name\n\n**Units of observation**:\n\n- `COUNTY`: US County\n- `STATE`: US State\n- `CASE`: Realized case (in practice, we would give this a more specific definition. What defines a case? What sort of confirmation is required? Is the event recorded on the date or realization or the date of reporting?)\n- `HOSP`: Realized hospitalization (same note as above)\n- `DEATH`: Realized death (same note as above)\n\n**Descriptors**:\n\n- `ACTL`: Actual observed value\n- `PRED`: Predicted value\n- `HPSA`: Health Professional Shortage Area (county-level measure)\n\n### Data Sources and Flow\n\nOur goal is to end up with a `model_monitor` table with one record per `observation date` and `county` (same as the `actual` table). Using the grammar above, we may define the variables we intend to include in our final table:\n\n- `CD_(COUNTY|STATE)`: Unique county/state identifier (from Census Bureau FIPS codes)\n- `NM_(COUNTY|STATE)`: Human-readable county/state names- \n- `DT_COUNTY`: The date a county's values are observed\n- `N_(CASE|HOSP|DEATH)_(ACTL|PRED)_(07|14|21|28)`: The actual or predicted number of cases, hospitalizations, or deaths (and, for predictions only, the value of these predictions at 7, 14, 21, and 28 days prior to the day being forecasted)\n- `IND_COUNTY_HPSA`: Indicator of whether county is considered a shortage area\n- `PROP_COUNTY_HPSA`: Proportion of population that is underserved in a designated shortage area \n\nWe will source these fields from four tables:\n\n- `actual` table \n  + sourced from `bigquery-public-data`.`covid19_jhu_csse`.`summary`\n  + one record per `observation date` x `county`\n  + fields for county code, observation date, realized number of cases and deaths\n- `prediction` table\n  + sourced from `bigquery-public-data`.`covid19_public_forecasts`.`county_28d_historical`\n  + one record per `date prediction was made` x `data being predicted` x `county` (initially)\n  + fields for county code, observation date, prediction date, predicted number of cases and deaths\n  + we transform to one record per `observation date` x `county` with observations at different time lags represented as separate fields\n- `hpsa` table\n  + sourced from `bigquery-public-data`.`sdoh_hrsa_shortage_areas`.`hpsa_primary_care`\n  + (after some wrangling on our end) one record per `county` for counties identified as having a shortage\n  + fields for the county code, date of designation, proportion of county under-served\n- `fips` table^[Technically, this table should be static, so the same information could be included with `dbt`'s [Seeds](https://docs.getdbt.com/docs/building-a-dbt-project/seeds) feature]\n  + sourced from `bigquery-public-data`.`census_utility`.`fips_codes_all`\n  + (after some wrangling) one record per `county` for each county in the 50 US states\n  + fields for FIPS code (Census Bureau county identifiers), state name, county name\n\nFor a conceptual mental map, once all the wrangling and cleaning is done for each of the tables above, we might have psuedocode for the final table that looks something like this.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nselect *\nfrom \n  actual \n    left join\n  predictions using (cd_county, dt_county)\n    left join\n  hpsa using (cd_county)\n    left join\n  fips using (cd_county)\n```\n:::\n\n\nBut as we're about to see, `dbt` allows us to get a bit more complex and elegant.\n\n## Variable Creation with Jinja Templating\n\n`dbt` makes it easy to create typo-free variable names that adhere to our controlled vocabulary by using the Jinja templating language.^[For another exploration of using Jinja templating to generate SQL, check out this nice [blog post](https://multithreaded.stitchfix.com/blog/2017/07/06/one-weird-trick/) from Stitch Fix] Jinja brings traditional control-flow elements like conditional statements and loops to make SQL more programmatic. When `dbt` is executed with `dbt run`, it first renders this Jinja to standard SQL before sending the query to the database. \n\nTemplates, and specifically loops, help write more concise and proof-readable SQL code when deriving a large number of variables with similar logic. For example, below we collapse the raw prediction data (which is represented as one record for `each county` x `each day being prediction` x `each day a prediction was made`) to one record for each county and each day being predicted with different columns containing the numeric value of each prediction of cases, hospitalizations, and deaths at `lags` (defined in the `dbt_project.yml` configuration file) of 7, 14, 21, and 28 days prior to the date being predicted. \n\nOrdinarily, deriving these 12 variables (3 measures x 4 lags) would pose significant room for typos in either the code or the variable names, but in this script, the Jinja template of `n_case_pred_{{l}}` ensures consistency.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n{{\n    config(\n        materialized='incremental',\n        unique_key= 'id'\n    )\n}}\n\nselect\n  county_fips_code || ' ' || forecast_date as id,\n  county_fips_code as cd_county,\n  forecast_date as dt_county,\n  {% for l in var('lags') %}\n    max(if(date_diff(prediction_date, forecast_date, day) = {{l}}, \n         round(100*new_confirmed, 0), null)) as n_case_pred_{{l}},\n    max(if(date_diff(prediction_date, forecast_date, day) = {{l}}, \n         round(100*hospitalized_patients, 0), null)) as n_hosp_pred_{{l}},\n    max(if(date_diff(prediction_date, forecast_date, day) = {{l}}, \n         round(100*new_deaths, 0), null)) as n_death_pred_{{l}}\n  {% if not loop.last %},{% endif %}\n  {% endfor %}\nfrom {{ source('bqpred', 'pred') }}\nwhere \n  cast(left(county_fips_code, 2) as int64) between 1 and 56 and\n  forecast_date <= current_date()\n  {% if is_incremental() %}\n  and forecast_date >= (\n    select dateadd(day, -7, max(dt_county)) from {{this}}\n  )\n  {% endif %}\ngroup by 1,2,3\n```\n:::\n\n\nThis script renders to the following:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nselect\n  county_fips_code || ' ' || forecast_date as id,\n  county_fips_code as cd_county,\n  forecast_date as dt_county,\n  \n    max(if(date_diff(prediction_date, forecast_date, day) = 07, \n         round(100*new_confirmed, 0), null)) as n_case_pred_07,\n    max(if(date_diff(prediction_date, forecast_date, day) = 07, \n         round(100*hospitalized_patients, 0), null)) as n_hosp_pred_07,\n    max(if(date_diff(prediction_date, forecast_date, day) = 07, \n         round(100*new_deaths, 0), null)) as n_death_pred_07\n  ,\n  \n    max(if(date_diff(prediction_date, forecast_date, day) = 14, \n         round(100*new_confirmed, 0), null)) as n_case_pred_14,\n    max(if(date_diff(prediction_date, forecast_date, day) = 14, \n         round(100*hospitalized_patients, 0), null)) as n_hosp_pred_14,\n    max(if(date_diff(prediction_date, forecast_date, day) = 14, \n         round(100*new_deaths, 0), null)) as n_death_pred_14\n  ,\n  \n    max(if(date_diff(prediction_date, forecast_date, day) = 21, \n         round(100*new_confirmed, 0), null)) as n_case_pred_21,\n    max(if(date_diff(prediction_date, forecast_date, day) = 21, \n         round(100*hospitalized_patients, 0), null)) as n_hosp_pred_21,\n    max(if(date_diff(prediction_date, forecast_date, day) = 21, \n         round(100*new_deaths, 0), null)) as n_death_pred_21\n  ,\n  \n    max(if(date_diff(prediction_date, forecast_date, day) = 28, \n         round(100*new_confirmed, 0), null)) as n_case_pred_28,\n    max(if(date_diff(prediction_date, forecast_date, day) = 28, \n         round(100*hospitalized_patients, 0), null)) as n_hosp_pred_28,\n    max(if(date_diff(prediction_date, forecast_date, day) = 28, \n         round(100*new_deaths, 0), null)) as n_death_pred_28\n  \n  \nfrom `bigquery-public-data`.`covid19_public_forecasts`.`county_28d_historical`\nwhere \n  cast(left(county_fips_code, 2) as int64) between 1 and 56 and\n  forecast_date <= current_date()\n  \ngroup by 1,2,3\n```\n:::\n\n\nThis script and the other three that derive our base tables (`actual`, `prediction`, `fips`, and `hpsa`) can be found in [the `models` directory](https://github.com/emilyriederer/dbt-convo-covid/tree/main/models) of the repo. After they are individually created, they are combined into the `model_monitor_staging` table in the relatively uninteresting [script](https://github.com/emilyriederer/dbt-convo-covid/blob/main/models/model_monitor_staging.sql):\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n{{\n    config(\n        materialized='incremental',\n        unique_key='id'\n    )\n}}\n\nselect\n  actual.*,\n  prediction.* except (cd_county, dt_county, id),\n  fips.* except (cd_county),\n  hspa.* except (cd_county)\nfrom\n  {{ ref('actual') }} as actual\n  inner join\n  {{ ref('prediction') }} as prediction\n  using (dt_county, cd_county)\n  left join\n  {{ ref('fips') }} as fips\n  using (cd_county)\n  left join\n  {{ ref('hpsa') }} as hspa\n  using (cd_county)\n{% if is_incremental() %}\nwhere dt_county >= (\n  select dateadd(day, -7, max(dt_county)) from {{this}}\n  )\n{% endif %}\n```\n:::\n\n\n## Variable Manipulation with Regex Macros\n\nOf course, it's not enough to adhere to controlled vocabulary *naming*. If the actual *contracts* implied in those names are not upheld, the process is meaningless (or, worse, dangerous). When preparing our final table, we want to explicitly enforce as many of the vocabulary's promises to be met as possible. This means, for example, ensuring all variables prefixed with `n` are really integers, `dt` are truly dates (and not just similarly formatted strings), and `ind` variables are actually never-null.\n\nThis time, we again use Jinja templating along with another dbt feature: custom macros. The final script in our pipeline ([`model_monitor`](https://github.com/emilyriederer/dbt-convo-covid/blob/main/models/model_monitor.sql)) uses custom macros `get_column_names()` to determine all of the column names in the staging table and `get_matches()` to subset this list for variable names which match regular expressions corresponding to different prefixes. \n\nThen, we iterate over each of these lists to apply certain treatments to each set of columns such as casting `cols_n` and `cols_dt` variables to `int64` and `date` respectively, rounding `cols_prop` variables to three decimal places, and coalescing `cols_ind` variables to be 0 if null.^[Ordinarily, we would want to be careful setting null values to 0. We would not want to lie and imply the existence of missing data to nominally uphold a contract. However, this is the correct approach here. Our indicator variables in this case come from tables which only contain the `1` or \"presence\" values (e.g. the `hpsa` relation which provides `ind_county_hpsa` only has records for counties which are shortage areas) so this is a safe approach.] \n\n\n::: {.cell}\n\n```{.sql .cell-code}\n{{\n    config(\n        materialized='incremental',\n        unique_key='id',\n\t\tpartition_by={\n\t\t  \"field\": \"dt_county\",\n\t\t  \"data_type\": \"date\",\n\t\t  \"granularity\": \"month\"\n\t\t}\n    )\n}}\n\n{% set cols = get_column_names( ref('model_monitor_staging') ) %}\n{% set cols_n = get_matches(cols, '^n_.*') %}\n{% set cols_dt = get_matches(cols, '^dt_.*') %}\n{% set cols_prop = get_matches(cols, '^prop_.*') %}\n{% set cols_ind = get_matches(cols, '^ind_.*') %}\n{% set cols_oth = cols\n   | reject('in', cols_n)\n   | reject('in', cols_dt)\n   | reject('in', cols_prop)\n   | reject('in', cols_ind) %}\n\nselect\n\t\n   {%- for c in cols_oth %}\n   {{c}},\n   {% endfor -%}\n   {%- for c in cols_n %} \n     cast({{c}} as int64) as {{c}}, \n   {% endfor %}\n   {%- for c in cols_dt %} \n     date({{c}}) as {{c}}, \n   {% endfor -%}\n   {%- for c in cols_prop %} \n     round({{c}}, 3) as {{c}}, \n   {% endfor -%}\n   {%- for c in cols_ind %} \n     coalesce({{c}}, 0) as {{c}} \n     {% if not loop.last %},{% endif %} \n   {% endfor -%}\n   \nfrom {{ ref('model_monitor_staging') }}\n\n{% if is_incremental() %}\nwhere dt_county >= (\n  select dateadd(day, -7, max(dt_county)) from {{this}}\n  )\n{% endif %}\n```\n:::\n\n\nNote how abstract this query template is. In fact, it completely avoids referencing specific variables in our table.^[In fact, this could also be a macro, as I introduce before, and shipped in a package to apply across all data models in an analytical database. To make the narrative of this example easier to follow, I leave it as a standard query model.] If we should decide to go back and add more fields (for example, actual and predicted recoveries) into our upstream models, they will receive the correct post-processing and validation as long as they are named appropriately.\n\nFor a peak under the hood, here's how those two macros work.\n\nFirst, `get_column_names()` simply queries the databases' built in [`INFORMATION_SCHEMA`](https://en.wikipedia.org/wiki/Information_schema)^[An automatically created table containing metadata such as field names and types for each table in a database] to collect all column names of a given table. In the case of the `model_monitor.sql` script, the table provided is the staging table (`model_monitor_staging`) which was made in the previous step.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n{% macro get_column_names(relation) %}\n\n{% set relation_query %}\nselect column_name\nFROM {{relation.database}}.{{relation.schema}}.INFORMATION_SCHEMA.COLUMNS\nWHERE table_name = '{{relation.identifier}}';\n{% endset %}\n\n{% set results = run_query(relation_query) %}\n\n{% if execute %}\n{# Return the first column #}\n{% set results_list = results.columns[0].values() %}\n{% else %}\n{% set results_list = [] %}\n{% endif %}\n\n{{ return(results_list) }}\n\n{% endmacro %}\n```\n:::\n\n\nNext, the `get_matches()` macro simply iterates through a list of characters (such as the column names obtained in the previous step) and appends only those that match our regex to the final list that is returned.^[For those interested in the nitty gritty details, we must loop here because Jinja does not allow the more compact python list comprehensions. Additionally, Jinja only allows the python `append` method in display brackets `{{}}` so the `or ''` is a trick to silence the output, per [this site](http://svn.python.org/projects/external/Jinja-2.1.1/docs/_build/html/faq.html#isn-t-it-a-terrible-idea-to-put-logic-into-templates).] ^[Note that if you have installed dbt previously, this solution might not work for you. The python `re` library for regular expressions was not enabled inside dbt's Jinja until the recent release of [v0.19.0](https://github.com/fishtown-analytics/dbt/releases/tag/v0.19.0) ] (Thanks to [David Sanchez](https://twitter.com/dsmd4vid) on the `dbt` Slack community for helping me figure out how to call the `re` library from within Jinja.)\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n{% macro get_matches(input_list, regex) %}\n\n{% set results_list = [] %}\n{% for l in input_list %}\n    {% if modules.re.match(regex, l, modules.re.IGNORECASE) %}\n        {{ results_list.append(l) or \"\" }}\n    {% endif %}\n{% endfor %}\n\n{{ return(results_list) }}\n\n{% endmacro %}\n```\n:::\n\n\nThese macros live in the [`macros/` directory](https://github.com/emilyriederer/dbt-convo-covid/tree/main/macros) of the repository.\n\n## Data Validation with Custom Tests\n\nOf course, not every contract can be made by force without risk of corrupting data. For any that we cannot enforce in their creation, we must rigorously test.\n\n`dbt`'s testing framework allows for testing any data model in the project -- not just the final table. This is very useful to intercept errors as soon as they happen instead of trying to backtrack from bad output many steps later. Some tests are built-in but others can be custom written as SQL `SELECT` statements.\n\nBuilt-in tests for properties of individual columns include `unique`, `not_null`, and `relationship`^[The add-on package `dbt-utils` contains many more common tests such as `unique_combination`, `not_null_where`, etc.]. These can be implemented in the `schema.yml` configuration file under the `tests` key-value pair for each relevant column, and can sometimes be shared across models with the YAML `&` and `*` (as shown below with the same `basetest` checks being applied to the `actual` and `prediction` data models) which allows for naming and repeating blocks (think copy-paste).  However, even with a relatively small number of tests and columns, its cumbersome and easy to overlook a column.\n\n\n::: {.cell}\n\n```{.yaml .cell-code}\nversion: 2\n\nsources:\n  - name: bqhspa\n    description: HRSA designated shortage areas\n    database: bigquery-public-data\n    schema: sdoh_hrsa_shortage_areas\n    tables:\n      - name: hpsa\n        identifier: hpsa_primary_care\n  - name: bqcensus\n    description: > \n      Census Bureau mapping of FIPS codes to county and state names\n    database: bigquery-public-data\n    schema: census_utility\n    tables:\n      - name: fips\n        identifier: fips_codes_all\n  - name: bqjhu\n    description: > \n      Daily COVID case and death statistics by county \n      from the Johns Hopkins University CSSE\n    database: bigquery-public-data\n    schema: covid19_jhu_csse\n    tables:\n      - name: actual\n        identifier: summary    \n  - name: bqpred\n    description: Forecasted case and death statistics\n    database: bigquery-public-data\n    schema: covid19_public_forecasts\n    tables:\n      - name: pred\n        identifier: county_28d_historical   \n\nmodels:\n  - name: actual\n    description: >\n      Actual COVID cases and deaths by county\n    columns: &basetest\n      - name: id\n        tests:\n          - unique\n          - not_null\n      - name: cd_county\n        tests:\n          - relationships:\n              to: ref('fips')\n              field: cd_county\n  - name: prediction\n    description: > \n      Predicted COVID cases and deaths by county\n    columns: *basetest\n  - name: hpsa\n    description: >\n      Counties designated as healthcare shortage areas\n    columns:\n      - name: cd_county\n        tests:\n          - unique\n          - not_null\n          - relationships:\n              to: ref('fips')\n              field: cd_county\n  - name: fips\n    description: > \n      Mapping of county and state names from FIPS codes\n    columns:\n      - name: cd_county\n        tests:\n          - unique\n          - not_null \n  - name: model_monitor_staging\n    description: >\n      Staging table to combine different data sources\n  - name: model_monitor\n    description: >\n      Final model monitoring table with one row per county x observed day\n    columns:\n      - name: id\n        test:\n        - unique\n        - not_null\n      - name: ind_county_hpsa\n        tests:\n        - not_null\n        - accepted_values:\n            values: [0,1]  \n            quote: false   \n      - name: prop_county_hpsa\n        tests:\n          - dbt_utils.not_null_where:\n              where: \"ind_county_hpsa = 1\"\n```\n:::\n\n\nInstead, developers may also define custom tests as SQL `SELECT` statements which returns only records that fail the test. Like data models, tests may also use Jinja and macros. This allows us to abstract some of our data validation tests to target all variables with a specific naming convention (and, thus, performance contract) at any arbitrary point in the pipeline. \n\nFor example, in the `model_monitor` data model shown in the last section, we explicitly cast all variables that start with `n` to be integers. However, before we do this, we should probably ensure that these fields are truly \"integer-like\"; otherwise, if we are casting values that have unexpected fractional components, we are simply masking inaccurate data.\n\nThe following test checks whether the `n` variables in the `model_monitor_staging` table (before casting) are sufficiently \"integer like\". It first retrieves all fields in this tables, next subsets all field names only to those with `n` prefixes, and finally uses Jinja to create a SQL script with separate `WHERE` conditions to check if the absolute difference between each `n` variable and its value after being cast to an integer is ever greater than 0.01 (which would imply a violation.)\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n{% set cols = get_column_names( ref('model_monitor_staging') ) %}\n{% set cols_n = get_matches(cols, '^n_.*') %}\n\nselect *   \nfrom {{ ref('model_monitor_staging') }}\nwhere\n   {%- for c in cols_n %} abs({{c}} - cast({{c}} as int64)) > 0.01 or \n   {% endfor %}\n   FALSE\n```\n:::\n\n\nWe can apply the same trick to testing more conditions on the final table. For example, the following test checks whether every `prop` variable is truly bounded between 0 and 1 (by returning any times where this is *not* the case.)\n\n\n::: {.cell}\n\n```{.sql .cell-code}\n{% set cols = get_column_names( ref('model_monitor') ) %}\n{% set cols_n = get_matches(cols, '^prop_.*') %}\n\nselect *   \nfrom {{ ref('model_monitor') }}\nwhere\n   {%- for c in cols_n %} ({{c}} < 0 or {{c}} > 1) or \n   {% endfor %}\n   FALSE\n   \n```\n:::\n\n\nFinally, we may also use tests to ensure our naming conventions are upheld. The following script once again calls the `INFORMATION_SCHEMA` table (as did our `get_column_names()` macro) to obtain a table with one record for each column name in the final table. It next uses the `regexp_extract()` SQL function with capturing groups to create separate columns (`l1`, `l2`, `l3`) for each underscore-delimited section of the naming. Finally, the `WHERE` conditions filter the output for any stubs that do not match the convention.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nwith cols as (\nselect \n  column_name, \n  regexp_extract(lower(column_name), '^[a-z]+') as l1,\n  regexp_extract(lower(column_name), '^[a-z]+_([a-z]+)') as l2,\n  regexp_extract(lower(column_name), '^[a-z]+_[a-z]+_([a-z]+)') as l3\nfrom \n  {{ ref('model_monitor').database }}.\n    {{ ref('model_monitor').schema }}.\n\t  INFORMATION_SCHEMA.COLUMNS\nwhere table_name = '{{ ref('model_monitor').identifier }}'\n)\n\nselect *\nfrom cols \nwhere \n  l1 not in ('id', 'cd', 'n', 'nm', 'prop', 'pct', 'dt', 'ind') or \n  l2 not in ('county', 'state', 'case', 'hosp', 'death') or \n  l3 not in ('hpsa','pred', 'actl')\n```\n:::\n\n\nWe could further extend the script above and impose a *hierarchy* on our controlled vocabulary by adding additional conditions to the `WHERE` clause. For example, since the `HPSA` stub only makes sense as a suffix to `COUNTY` (e.g. there's no such thing as a health professional shortage area *case* or *death*), we could add the additional condition `or (l3 = 'hpsa' and not l2 = 'county')`.\n\nSimilarly, we can query the `INFORMATION_SCHEMA` to validate that each column has its implied data type.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nwith cols_type as (\nselect distinct \n  regexp_extract(lower(column_name), '^[a-z]+') as stub,\n  data_type\nfrom \n  {{ ref('model_monitor').database }}.\n    {{ ref('model_monitor').schema }}.\n\t  INFORMATION_SCHEMA.COLUMNS\nwhere table_name = '{{ ref('model_monitor').identifier }}'\n)\n\nselect * \nfrom cols_type\nwhere \n    (stub in ('id', 'cd', 'nm') and not data_type = 'STRING') or \n    (stub in ('n', 'ind') and not data_type = 'INT64') or \n    (stub in ('prop', 'pct') and not data_type = 'FLOAT64') or\n    (stub = 'dt' and not data_type = 'DATE')\n```\n:::\n\n\nAs with our `model_monitor.sql` data model, the beauty of these tests is that they have abstracted away the column names themselves. So, they will continue to test all of the correct pieces of intent regardless of whether columns are added or removed from the table. Like macros, these could also be put into a package so that the same tests could be applied to all tables in a database. \n\nThe code for these tests, and a few more similar examples, are located in the [`tests/` directory](https://github.com/emilyriederer/dbt-convo-covid/tree/main/tests) of the repository. They can be run on the command line with the `dbt test` command.\n\n## Sample Output\n\nTo conclude, I show a few top rows of output from the final model monitoring table:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nselect * \nfrom dbt_emily.model_monitor\nlimit 5\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|cd_county |dt_county  |id                  |cd_state |nm_county      |nm_state | n_case_actl| n_death_actl| n_case_pred_07| n_hosp_pred_07| n_death_pred_07| n_case_pred_14| n_hosp_pred_14| n_death_pred_14| n_case_pred_21| n_hosp_pred_21| n_death_pred_21| n_case_pred_28| n_hosp_pred_28| n_death_pred_28|dt_county_hpsa |prop_county_hpsa | ind_county_hpsa|\n|:---------|:----------|:-------------------|:--------|:--------------|:--------|-----------:|------------:|--------------:|--------------:|---------------:|--------------:|--------------:|---------------:|--------------:|--------------:|---------------:|--------------:|--------------:|---------------:|:--------------|:----------------|---------------:|\n|01001     |2021-08-15 |2021-08-15 01:00:01 |01       |Autauga County |Alabama  |        8025|          114|           1900|           1355|               8|           1634|           1537|               9|           1426|           1561|               9|           1260|           1492|               8|NA             |NA               |               0|\n|01001     |2021-01-02 |2021-01-02 01:00:01 |01       |Autauga County |Alabama  |        4268|           50|           2323|           2215|              29|           1768|           1942|              26|           1434|           1625|              22|           1214|           1333|              18|NA             |NA               |               0|\n|01001     |2021-06-07 |2021-06-07 01:00:01 |01       |Autauga County |Alabama  |        7206|          113|            758|            514|              14|            483|            466|              13|            308|            425|              12|            196|            385|              10|NA             |NA               |               0|\n|01001     |2020-11-24 |2020-11-24 01:00:01 |01       |Autauga County |Alabama  |        2661|           39|           2668|           1253|              14|           2939|           1375|              15|           3200|           1510|              17|           3461|           1652|              18|NA             |NA               |               0|\n|01001     |2021-08-22 |2021-08-22 01:00:01 |01       |Autauga County |Alabama  |        8311|          115|           1833|           2429|              13|           1680|           2740|              14|           1561|           2871|              14|           1461|           2877|              14|NA             |NA               |               0|\n:::\n:::\n\n\n## Bonus - Analysis Prep with Jinja Templates\n\nAlthough this post primarily focuses on uses of `dbt` to help data producers apply controlled vocabularies, dbt also provides an interesting framework for transitioning projects to data consumers with the use of their [Analyses](https://docs.getdbt.com/docs/building-a-dbt-project/analyses) feature. Analyses are additional SQL script templates that are not sent to the database to produce tables or views.Instead, running `dbt compile` simply renders these scripts for use in analyses or BI tools. \n\nFor example of an \"analysis\", and as another example of templating in action, the following script uses our published table to compute the percent difference between actual observations and each prediction.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nselect\n\n  {%- for l in var('lags') %}\n    {%- for m in ['case', 'death'] %}\n\t  case \n\t    when n_{{m}}_actl = 0 then null \n        else round( (n_{{m}}_actl - n_{{m}}_pred_{{l}}) / n_{{m}}_actl, 4)\n      end as pctdiff_{{m}}_pred_{{l}} ,  \n    {% endfor %}\n  {% endfor %}\n  \n  mm.*\n  \nfrom {{ ref('model_monitor') }} as mm\n```\n:::\n\n\nIt compiles to:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nselect\n\t  case \n\t    when n_case_actl = 0 then null \n        else round( (n_case_actl - n_case_pred_07) / n_case_actl, 4)\n      end as pctdiff_case_pred_07 ,  \n    \n\t  case \n\t    when n_death_actl = 0 then null \n        else round( (n_death_actl - n_death_pred_07) / n_death_actl, 4)\n      end as pctdiff_death_pred_07 ,  \n    \n  \n\t  case \n\t    when n_case_actl = 0 then null \n        else round( (n_case_actl - n_case_pred_14) / n_case_actl, 4)\n      end as pctdiff_case_pred_14 ,  \n    \n\t  case \n\t    when n_death_actl = 0 then null \n        else round( (n_death_actl - n_death_pred_14) / n_death_actl, 4)\n      end as pctdiff_death_pred_14 ,  \n    \n  \n\t  case \n\t    when n_case_actl = 0 then null \n        else round( (n_case_actl - n_case_pred_21) / n_case_actl, 4)\n      end as pctdiff_case_pred_21 ,  \n    \n\t  case \n\t    when n_death_actl = 0 then null \n        else round( (n_death_actl - n_death_pred_21) / n_death_actl, 4)\n      end as pctdiff_death_pred_21 ,  \n    \n  \n\t  case \n\t    when n_case_actl = 0 then null \n        else round( (n_case_actl - n_case_pred_28) / n_case_actl, 4)\n      end as pctdiff_case_pred_28 ,  \n    \n\t  case \n\t    when n_death_actl = 0 then null \n        else round( (n_death_actl - n_death_pred_28) / n_death_actl, 4)\n      end as pctdiff_death_pred_28 ,  \n    \n  \n  \n  mm.*\n  \nfrom `sonorous-wharf-302611`.`dbt_emily`.`model_monitor` as mm\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}