{
  "hash": "2f98292845414f649dfc5c481a5a7ba3",
  "result": {
    "markdown": "---\ntitle: \"Goin' to Carolina in my mind (or on my hard drive)\"\ndescription: \"Out-of-memory processing of North Carolina's voter file with DuckDB and Apache Arrow\"\nauthor: \"Emily Riederer\"\ndate: \"2022-09-25\"\ncategories: [data, sql]\nimage: \"featured.jpg\"\naliases:\n  - /post/duckdb-carolina/\n---\n\n\n![ Photo Credit to [Element5 Digital](https://unsplash.com/@element5digital) on Unsplash ](featured.jpg)\n\nThere comes a time in every analyst's life when data becomes too big for their laptop's RAM. While open-source tools like R, python, and SQL have made \"team of one\" data analysts ever more powerful, analysts abilities to derive value from their skillsets are highly interdependent with the tools at their disposal. \n\nFor R and python, the size of datasets becomes a limiting factor to local processing; for a SQL-focused analyst, the existence of a database is prerequisite, as the gap between \"democratized\" SQL *querying* skills and data engineering and database management skills is not insignificant. The ever-increasing number of managed cloud services (from data warehouses, containers, hosted IDEs and notebooks) offer a trendy and effective solution. However, budget constraints, technical know-how, security concerns, or tight-timelines can all be headwinds to adoption. \n\nSo what's an analyst to do when they have the knowledge and tools but not the infrastructure to tackle their problem? \n\n[`DuckDB`](https://DuckDB.org/) is quickly gaining popularity as a solution to some of these problems. DuckDB is a no-dependency, serverless database management system that can help parse massive amounts of data out-of-memory via familiar SQL, python, and R APIs. Key features include:\n\n- **Easy set-up**: Easily installed as an executable or embedded within R or python packages\n- **Columnar storage**: For efficient retrieval and vectorized computation in analytics settings\n- **No installation or infrastructure required**: Runs seamlessly on a local machine launched from an executable \n- **No loading required**: Can read external CSV and Parquet files *and* can smartly exploit Hive-partitioned Parquet datasets in optimization\n- **Expressive SQL**: Provides semantic sugar for analytical SQL uses with clauses like `except` and `group by all` (see blog [here](https://DuckDB.org/2022/05/04/friendlier-sql.html))\n\nThis combination of features can empower analysts to use what they have and what they know to ease into the processing of much larger datasets. \n\nIn this post, I'll walk through a scrappy, minimum-viable setup for analysts using `DuckDB`, motivated by the [North Carolina State Board of Election](https://www.ncsbe.gov/results-data)'s rich voter data. Those interested can follow along in [this repo](https://github.com/emilyriederer/nc-votes-DuckDB) and put it to the test by launching a free 8GB RAM GitHub Codespaces.\n\nThis is very much *not* a demonstration of best practices of anything. It's also not a technical benchmarking of the speed and capabilities of `DuckDB` versus alternatives. (That ground is well-trod. If interested, see [a head-to-head to pandas](https://DuckDB.org/2021/05/14/sql-on-pandas.html) or [a matrix of comparisons across database alternatives](https://benchmark.clickhouse.com/).) If anything, it is perhaps a \"user experience benchmark\", or a description of a minimum-viable set-up to help analysts use what they know to do what they need to do.\n\n## Motivation: North Carolina election data \n\nNorth Carolina (which began accepting ballots in early September for the upcoming November midterm elections) offers a rich collection of voter data, including daily-updating information on the current election, full voter registration data, and ten years of voting history. \n\n- NC 2022 midterm early vote data from [NCSBE](https://www.ncsbe.gov/results-data) (~6K records as-of 9/23 and growing fast!)\n- NC voter registration file from [NCSBE](https://www.ncsbe.gov/results-data) (~9M records / 3.7G unzipped, will be static for this cycle once registration closes in October)\n- NC 10-year voter history file from [NCSBE](https://www.ncsbe.gov/results-data) (~22M records / 5G unzipped, static)\n\nAll of these files are released as zipped full-population (as opposed to delta) CSV files.\n\nOne can imagine that this data is of great interest to campaign staff, political scientists, pollsters, and run-of-the-mill political junkies and prognosticators. However, the file sizes of registration and history data, which is critical for predicting turnout and detecting divergent trends, could be prohibitive. \n\nBeyond these files, analysis using this data could surely be enriched by additional third-party sources such as:\n\n- Current Population Survey 2022 November voting supplement from [US Census Bureau](https://www.census.gov/data/datasets/time-series/demo/cps/cps-supp_cps-repwgt/cps-voting.html)\n- County-level past election results from [MIT Election Lab via Harvard Dataverse](https://dataverse.harvard.edu/file.xhtml?fileId=6104822&version=10.0)\n- Countless other data sources either from the US Census, public or internal campaign polls, organization-specific mobilizaton efforts, etc.\n\nYour mileage may vary based on your system RAM, but many run-of-the-mill consumer laptops might struggle to let R or python load all of this data into memory. Or, a SQL-focused analyst might yearn for a database to handle all these complex joins.\n\nSo how can `DuckDB` assist?\n\n## DuckDB highlights\n\nTo explain, we'll first level-set with a brief demo of some of the most relevant features of `DuckDB`. \n\nSuppose we have flat files of data, like a `sample.csv` (just many orders of magnitude larger!)\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,2,3], 'b':[4,5,6], 'c':[7,8,9]})\ndf.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n```\n:::\n\n```{.python .cell-code}\ndf.to_csv('sample.csv', index = False)\n```\n:::\n\n\n`DuckDB` can directly infer it's schema and read it in a SQL-like interface by using functions like `read_csv_auto()` in the `FROM` clause.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport duckdb\ncon = duckdb.connect()\ndf = con.execute(\"select * from read_csv_auto('sample.csv')\").fetchdf()\ndf.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n```\n:::\n\n```{.python .cell-code}\ncon.close()\n```\n:::\n\n\nWhile very useful, this is of course bulky to type. We may also set-up a persistent DuckDB database as a `.duckdb` file as save tables with CTAS statements, as with any normal relational database. Below, we create the `sample-db.duckdb` database and add one table and one view with our data.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncon = duckdb.connect('sample-db.duckdb')\nctas = \"create or replace table sample as (select * from read_csv_auto('sample.csv'));\"\ncon.execute(ctas)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<duckdb.DuckDBPyConnection object at 0x0000000030026F70>\n```\n:::\n\n```{.python .cell-code}\ncvas = \"create or replace view sample_vw as (select * from read_csv_auto('sample.csv'));\" \ncon.execute(cvas)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<duckdb.DuckDBPyConnection object at 0x0000000030026F70>\n```\n:::\n\n```{.python .cell-code}\ncon.close()\n```\n:::\n\n\nNow, suppose the data in `sample.csv` changes (now with 4 rows versus 3).\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = pd.DataFrame({'a':[1,2,3,4], 'b':[4,5,6,7], 'c':[7,8,9,10]})\ndf.to_csv('sample.csv', index = False)\n```\n:::\n\n\nOur table stored the data directly within the database (\"disconnected\" from the file) so it remains the same as before whereas our view changed.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncon = duckdb.connect('sample-db.duckdb')\ndf1 = con.execute(\"select count(1) from sample\").fetchdf()\ndf2 = con.execute(\"select count(1) from sample_vw\").fetchdf()\ncon.close()\ndf1.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   count(1)\n0         3\n```\n:::\n\n```{.python .cell-code}\ndf2.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   count(1)\n0         4\n```\n:::\n:::\n\n\n(Here, I focus just on the features we will use; not strictly the coolest or most important. I highly encourage taking a spin through the [docs](https://duckdb.org/docs/guides/python/sql_on_pandas) for countless features not discussed -- like directly querying from or fetching to pandas and Arrow formats, an alternative relational API, etc.)\n\n## Data management pattern\n\nWith these features in mind, we return to the problem at hand. How can an analyst mimic the experience of having the infrastructure needed to do their work?\n\nOne approach could look something like the following. As a one-time exercise someone would:\n\n1. Download all relevant files\n1. (Optionally) Convert large, static files to Parquet versus CSV. DuckDB handles both well, but Parquet has some benefits that we'll discuss in the next section\n1. Create a DuckDB database with references to the files as `view`s\n\nThen, any analyst wanting to interact with the data could:\n\n1. Interact with DuckDB as with any database connection\n1. Whenever needed, re-download the files to the same name/directory to \"refresh\" the \"database\"\n\nThe [nc-votes-duckdb](https://github.com/emilyriederer/nc-votes-duckdb) GitHub repo shows this flow in practice. If you want to follow along, you can click `Code > Create codespaces on master` and follow the more detailed instructions in the `README.md` or at the bottom of this post.\n\n### One-time set-up\n\nThe scripts for the first set of steps are in the `etl` subdirectory. The e-step (extract) isn't all that interesting -- just some basic python scripts for downloading files from the internet, unzipping, and moving them around. These land the raw data in the `data/raw` subdirectory.\n\nData transformation mostly involves converting large CSVs to Parquet format (and dropping personally-identifying fields from the data on principle). As mentioned above, this step is optional but has some benefits. First, if one person is \"configuring\" a database for many analysts, Parquet compression makes files smaller for storage and sharing. Second, at query-time Parquet is:\n\n+ More reliably structured with a well-defined schema\n+ Faster to retrieve due to columnar storage\n+ Able to be pruned by a savvy database optimizer (when appropriately partitioned by columns relevant to common query patterns)\n\n[Conversion from CSV to Parquet](https://duckdb.org/docs/guides/import/parquet_export) itself can be done with DuckDB. However, as of writing, I don't believe that writing to a Hive-partitioned dataset is possible, so for this step, I used `pyarrow`, the python interface to [Apache Arrow](https://arrow.apache.org/) (another promising, memory-conserving data processing framework.)\n\nThis snippet from [etl/transform-register.py](https://raw.githubusercontent.com/emilyriederer/nc-votes-duckdb/master/etl/transform-register.py) demonstrates streaming a CSV by chunk and writing it out to county-level partitions: \n\n\n::: {.cell}\n\n```{.python .cell-code}\n# convert to hive-partitioned parquet\nif os.path.exists(path_temp):\n    shutil.rmtree(path_temp)\n\nwith csv.open_csv(path_raw, \n                  convert_options= opts_convr_reg, \n                  parse_options = opts_parse,\n                  read_options = opts_read_reg) as reader:\n\n    for next_chunk in reader:\n        if next_chunk is None:\n            break\n        tbl = pa.Table.from_batches([next_chunk])\n        pq.write_to_dataset(\n                tbl,\n                root_path = path_temp,\n                use_dictionary = cols_reg_dict,\n                partition_cols= ['county_id']\n        )\n```\n:::\n\n\n(Notably: counties are rather imbalanced in size and not the most important geography in many election contexts. This is for example purpose only, but partitions should always be picked based on how you expect to use the data. )\n\nOnce all the data in transformed, we can \"load\" our DuckDB database with relative-path references to our data. Again, this step can be done through any DuckDB API or the command line. Below, I use python in the [etl/load-db.py](https://github.com/emilyriederer/nc-votes-duckdb/blob/master/etl/load-db.py) to create the `nc.duckdb` database and create references to the different datasets.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport duckdb\nimport os\n\n# clean-up if already exists\nif os.path.exists('nc.duckdb'):\n  os.remove('nc.duckdb')\n\n# create new duckdb files \ncon = duckdb.connect('nc.duckdb')\n\n# generate SQL to register tables\ntemplate = \"\"\"\n  CREATE VIEW {view_name} as \n  (select * from read_parquet('{path}'{opts}))\n  \"\"\"\ndata_dict = {\n  'early_vote': 'data/early_vt.parquet',\n  'hist_gen': 'data/history_general/*/*.parquet',\n  'hist_oth': 'data/history_other/*/*.parquet',\n  'register': 'data/register/*/*.parquet',\n  'cps_suppl': 'data/cps_suppl.parquet'\n}\npartitioned = ['hist_gen', 'hist_pri', 'register']\n\nfor k,v in data_dict.items():\n\n  print(\"Loading {view_name} data...\".format(view_name = k))\n  opt = ', HIVE_PARTITIONING=1' if k in partitioned else ''\n  cvas = template.format(view_name = k, path = v, opts = opt)\n  con.execute(cvas)\n\ncon.close()\n```\n:::\n\n\nSimilarly, other views could be defined as desired that query these views to do further data transformation.\n\n### Ongoing usage\n\nDue to the decoupling of storage and compute, ongoing data management is nearly trivial. With this \"infrastructure\" set-up, analysts would need only to selectively redownload any changed datasets (in my project, using the `extract-.*.py` scripts as needed) to allow their queries to pull in the latest data.\n\nBig picture, that means that (after initial set-up) an analyst would have no more overhead \"managing their database\" than they would with a \"typical\" CSV-driven workflow. Specifically for this project, the early votes data is the only dataset that changes frequently. For ease-of-use, it could even be left in CSV format to make the download process even easier for any users. \n\n## Data access patterns\n\nWith this set-up in place, analysts can then use their favorite tools to query the data.\n\n### python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport duckdb\ncon = duckdb.connect('nc.duckdb')\ndf = con.execute('select count(1) from early_vote').fetchdf()\ncon.close()\n```\n:::\n\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckdb)\ncon <- dbConnect( duckdb('nc.duckdb') )\ndf <- dbGetQuery('select count(1) from early_vote')\ndbDisconnect(con, shutdown = TRUE)\n```\n:::\n\n\n### CLI\n\n```\nduckdb my-db.duckdb\n> select count(1) from early_vote\n```\n\n### SQL IDE (DBeaver)\n\nDuckDB also works with open-source database IDEs like [DBeaver](https://dbeaver.com/) for the full, \"traditional\" database experience. The [DuckDB website](https://duckdb.org/docs/guides/sql_editors/dbeaver) gives full set-up instructions. With DBeaver, analysts get the \"full\" database experience with navigable access to table schemas and metadata.\n\n![](dbeaver-query.png)\n\n![](dbeaver-er.png)\n\n\nNotably **if you are using relative file paths in your view definitions, you have to launch DBeaver from your command line after moving into the appropriate working directory**. (Thanks to [Elliana May on Twitter](https://twitter.com/Mause_me/status/1571126401482510336?s=20&t=uYOnuHSjZcjkrbwYb0aXvA) for the pointer.) (In the terminal: `cd my/dir/path; dbeaver`)\n\n## Codespaces Demo\n\nSo can DuckDB help analysts wrangle the whole state of North Carolina with 8GB RAM? To find out, launch a GitHub Codespaces from the [nc-votes-duckdb](https://github.com/emilyriederer/nc-votes-duckdb) repo and see for yourself! \n\n1. Launch on Codespaces\n\n2. Set-up environment:\n\n```\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install -r requirements.txt\n```\n\n3. Pull all raw data:\n\n```\nchmod +x etl/extract-all.sh\netl/extract-all.sh\n```\n\n4. Transform all raw data:\n\n```\nchmod +x etl/transform-all.sh\netl/transform-all.sh\n```\n\n5. Create duckdb database:\n\n```\npython etl/load-db.py\n```\n\n6. (Optional) Install duckdb CLI\n\n```\nchmod +x get-duckdb-cli.sh\n./get-duckdb-cli.sh\n```\n\n7. Run sample queries\n\n7a. Run sample queries in CLI\n\nLaunch the CLI:\n\n```\n./duckdb nc.duckdb\n.timer on\n```\n\n(Note: you can exit CLI with Ctrl+D)\n\nTry out some sample queries. For example, we might wonder how many past general elections that early voters have voted in before:\n\n```\nwith voter_general as (\nselect early_vote.ncid, count(1) as n\nfrom \n  early_vote \n  left join \n  hist_gen \n  on early_vote.ncid = hist_gen.ncid \ngroup by 1)\nselect n, count(1) as freq\nfrom voter_general\ngroup by 1\norder by 1\n;\n```\n\nAnd, this question is more interesting if we join on registration data to learn how many prior general elections each voter was eligible to vote in:\n\n```\nwith voter_general as (\nselect \n  early_vote.ncid, \n  extract('year' from register.registr_dt) as register_year, \n  count(1) as n\nfrom \n  early_vote \n  left join \n  hist_gen \n  on early_vote.ncid = hist_gen.ncid \n  left join\n  register\n  on early_vote.ncid = register.ncid\ngroup by 1,2)\nselect\n  n, \n  case \n  when register_year < 2012 then 'Pre-2012'\n  else register_year\n  end as register_year,\n  count(1) as freq\nfrom voter_general\ngroup by 1,2\norder by 1,2\n;\n```\n\n(Yes, of course *date* matters more than year here, etc. etc. This is purely to demonstrate `duckdb` not rigorous analysis!)\n\n7b. Run sample queries in python\n\nIn python: See sample queries in `test-query.py` file\n\n\n8. Run `free` in the terminal to marvel at what 8GB of RAM can do!\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}