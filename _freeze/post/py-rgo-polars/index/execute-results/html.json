{
  "hash": "e7851da592b92cccee4f9e977e9eadff",
  "result": {
    "markdown": "---\ntitle: \"`polars`' Rgonomic Patterns\"\ndescription: \"In this follow-up post to Python Rgonomics, we deep dive into some of the advanced data wrangling functionality in python's `polars` package to see how it's powertools like column selectors and nested data structures mirror the best of `dplyr` and `tidyr`'s expressive and concise syntax\"\nauthor: \"Emily Riederer\"\ndate: \"2024-01-13\"\ncategories: [rstats, python, tutorial]\nimage: \"featured.jpg\"\n---\n\n\n![ Photo credit to [Hans-Jurgen Mager](https://unsplash.com/@hansjurgen007) on Unsplash ](featured.jpg)\n\nA few weeks ago, I shared some [recommended modern python tools and libraries](/post/py-rgo/) that I believe have the most similar ergonomics for R (specifically `tidyverse`) converts. This post expands on that one with a focus on the `polars` library. \n\nAt the surface level, all data wrangling libraries have roughly the same functionality. Operations like selecting existing columns and making new ones, subsetting and ordering rows, and summarzing results is tablestakes. \n\nHowever, no one falls in love with a specific library because it has the best `select()` or `filter()` function the world has ever seen. It's the ability to easily do more complex transformations that differentiate a package expert versus novice, and the learning curve for everything that happens *after* the \"Getting Started\" guide ends is what can leave experts at one tool feeling so disempowered when working with another. \n\nThis deeper sense of intuition and fluency -- when your technical brain knows intuitively how to translate in code what your analytical brain wants to see in the data -- is what I aim to capture in the term \"ergonomics\". In this post, I briefly discuss the surface-level comparison but spend most of the time exploring the deeper similarities in the functionality and workflows enabled by `polars` and `dplyr`.\n\n## What are `dplyr`'s ergonomics?\n\nTo claim `polars` has a similar aesthetic and user experience as `dplyr`, we first have to consider what the heart of `dplyr`'s ergonomics actually is. The explicit design philosophy is described in the developers' writings on [tidy design principles](https://design.tidyverse.org/unifying.html), but I'll blend those official intended principles with my personal definitions based on the lived user experience. \n\n- Consistent: \n  + Function names are highly consistent (e.g. snake case verbs) with dependable inputs and outputs (mostly dataframe-in dataframe-out) to increase intuition, reduce mistakes, and eliminate surprises\n  + Metaphors extend throughout the codebase. For example `group_by()` + `summarize()` or `group_by()` + `mutate()` do what one might expect (aggregation versus a window function) instead of requiring users to remember arbitrary command-specific syntax\n  + Always returns a new dataframe versus modifying in-place so code is more idempotent^[Meaning you can't get the same result twice because if you rerun the same code the input has already been modified] and less error prone\n- Composable: \n  + Functions exist at a \"sweet spot\" level of abstraction. We have the right primitive building blocks that users have full control to do anything they want to do with a dataframe but almost never have to write brute-force glue code. These building blocks can be layered however one choose to conduct\n  + Conistency of return types leads to composability since dataframe-in dataframe-out allows for chaining\n- Human-Centered:\n  + Packages hit a comfortable level of abstraction somewhere between fully procedural (e.g. manually looping over array indexes without a dataframe abstraction) and fully declarative (e.g. SQL-style languages where you \"request\" the output but aspects like the order of operations may become unclear). Writing code is essentially articulating the steps of an analysis\n  + This focus on code as recipe writing leads to the creation of useful optional functions and helpers (like my favorite -- column selectors)\n  + User's rarely need to break the fourth wall of this abstraction-layer (versus thinking about things like indexes in `pandas`)\n\n TLDR? We'll say `dplyr`'s ergonomics allow users to express complex transformation precisely, concisely, and expressively.\n\nSo, with that, we will import `polars` and get started!\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl\n```\n:::\n\n\nThis document was made with `polars` version `0.20.4`.\n\n## Basic Functionality\n\nThe similarities between `polars` and `dplyr`'s top-level API are already well-explored in many posts, including those by [Tidy Intelligence](https://blog.tidy-intelligence.com/posts/dplyr-vs-polars/) and [Robert Mitchell](https://robertmitchellv.com/blog/2022-07-r-python-side-by-side/r-python-side-by-side.html). \n\nWe will only do the briefest of recaps of the core data wrangling functions of each and how they can be composed in order to make the latter half of the piece make sense. We will meet these functions again in-context when discussing `dplyr` and `polar`'s more advanced workflows.\n\n### Main Verbs\n\n`dplyr` and `polars` offer the same foundational functionality for manipulating dataframes. Their APIs for these operations are substantially similar. \n\nFor a single dataset:\n\n- Column selection: `select()` -> `select()` + `drop()`\n- Creating or altering columns: `mutate()` -> `with_columns()`\n- Subsetting rows: `filter()` -> `filter()`\n- Ordering rows: `arrange()` -> `sort()`\n- Computing group-level summary metrics: `group_by()` + `summarize()` -> `group_by()` + `agg()`\n\nFor multiple datasets:\n\n- Merging on a shared key: `*_join()` -> `join(strategy = '*')`\n- Stacking datasets of the same structure: `union()` -> `concat()`\n- Transforming rows and columns: `pivot_{longer/wider}()`^[Of the `tidyverse` funtions mentioned so far, this is the only one found in `tidyr` not `dplyr`] -> `pivot()`\n\n### Main Verb Design\n\nBeyond the similarity in naming, `dplyr` and `polars` top-level functions are substantially similar in their deeper design choices which impact the ergonomics of use:\n\n- Referencing columns: Both make it easy to concisely references columns in a dataset without the repeated and redundant references to said dataset (as sometimes occurs in base R or python's `pandas`). dplyr does this through nonstandard evaluation wherein a dataframe's coumns can be reference directly within a data transformation function as if they were top-level variables; in `polars`, column names are wrapped in `pl.col()`\n- Optional argument: Both tend to have a wide array of nice-to-have optional arguments. For example the joining capabilities in both libraries offer optional join validation^[That is, validating an assumption that joins should have been one-to-one, one-to-many, etc.] and column renaming by appended suffix\n- Consistent dataframe-in -> dataframe-out design: `dplyr` functions take a dataframe as their first argument and return a dataframe. Similarly, `polars` methods are called on a dataframe and return a dataframe which enables the chaining workflow discussed next\n\n### Chaining (Piping)\n\nThese methods are applied to `polars` dataframes by *chaining* which should feel very familiar to R `dplyr` fans. \n\nIn `dplyr` and the broad `tidyverse`, most functions take a dataframe as their first argument and return a dataframe, enabling the piping of functions. This makes it easy to write more human-readable scripts where functions are written in the order of execution and whitespace can easily be added between lines. The following lines would all be equivalent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransformation2(transformation1(df))\n\ndf |> transformation1() |> transformation2()\n\ndf |>\n  transformation1() |>\n  transformation2()\n```\n:::\n\n\nSimilarly, `polars`'s main transfomration methods offer a consistent dataframe-in dataframe-out design which allows *method chaining*. Here, we similarly can write commands in order where the `.` beginning the next method call serves the same purpose as R's pipe. And for python broadly, to achieve the same affordance for whitespace, we can wrap the entire command in parentheses. \n\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n  df\n  .transformation1()\n  .transformation2()\n)\n```\n:::\n\n\nOne could even say that `polars` dedication to chaining goes even deeper than `dplyr`. In `dplyr`, while core dataframe-level functions are piped, functions on specific columns are still often written in a nested fashion^[However, this is more by convention. There's not a strong reason why they would strictly need to be.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>% mutate(z = g(f(a)))\n```\n:::\n\n\nIn contrast, most of `polars` column-level transformation methods also make it ergonomic to keep the same literate left-to-right chaining within column-level definitions with the same benefits to readability as for dataframe-level operations. \n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.with_columns(z = pl.col('a').f().g())\n```\n:::\n\n\n## Advanced Wrangling\n\nBeyond the surface-level similarity, `polars` supports some of the more complex ergonomics that `dplyr` users may enjoy. This includes functionality like:\n\n- expressive and explicit syntax for transformations across multiple rows\n- concise helpers to identify subsets of columns and apply transformations\n- consistent syntax for window functions within data transformation operations\n- the ability to work with nested data structures\n\nBelow, we will examine some of this functionality with a trusty fake dataframe.^[I recently ran a [Twitter poll](https://twitter.com/EmilyRiederer/status/1744707632886095998) on whether people prefer real, canonical, or fake datasets for learning and teaching. Fake data wasn't the winner, but a strategy I find personally fun and useful as the unit-test analog for learning.] As with `pandas`, you can make a quick dataframe in `polars` by passing a dictionary to `pl.DataFrame()`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl \n\ndf = pl.DataFrame({'a':[1,1,2,2], \n                   'b':[3,4,5,6], \n                   'c':[7,8,9,0]})\ndf.head()\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th><th>c</th></tr><tr><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>3</td><td>7</td></tr><tr><td>1</td><td>4</td><td>8</td></tr><tr><td>2</td><td>5</td><td>9</td></tr><tr><td>2</td><td>6</td><td>0</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### Explicit API for row-wise operations\n\nWhile row-wise operations are relatively easy to write ad-hoc, it can still be nice semantically to have readable and stylistically consistent code for such transformations. \n\n`dplyr`'s [`rowwise()`](https://dplyr.tidyverse.org/articles/rowwise.html) eliminates ambiguity in whether subsequent functions should be applied element-wise or collectively. Similiarly, `polars` has explicit `*_horizontal()` functions.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.with_columns(\n  b_plus_c = pl.sum_horizontal(pl.col('b'), pl.col('c')) \n)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th><th>c</th><th>b_plus_c</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>3</td><td>7</td><td>10</td></tr><tr><td>1</td><td>4</td><td>8</td><td>12</td></tr><tr><td>2</td><td>5</td><td>9</td><td>14</td></tr><tr><td>2</td><td>6</td><td>0</td><td>6</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### Column Selectors\n\n`dplyr`'s [column selectors](https://dplyr.tidyverse.org/reference/select.html) dynamically determine a set of columns based on pattern-matching their names (e.g. `starts_with()`, `ends_with()`), data types, or other features. I've previously [written](/post/column-name-contracts/) and [spoken](/talk/col-names-contract/) at length about how transformative this functionality can be when paired with \n\n`polars` has a similar set of [column selectors](https://docs.pola.rs/py-polars/html/reference/selectors.html). We'll import them and see a few examples.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars.selectors as cs\n```\n:::\n\n\nTo make things more interesting, we'll also turn one of our columns into a different data type.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = df.with_columns(pl.col('a').cast(pl.Utf8))\n```\n:::\n\n\n#### In `select`\n\nWe can select columns based on name or data type and use one or more conditions.\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.select(cs.starts_with('b') | cs.string())\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>b</th><th>a</th></tr><tr><td>i64</td><td>str</td></tr></thead><tbody><tr><td>3</td><td>&quot;1&quot;</td></tr><tr><td>4</td><td>&quot;1&quot;</td></tr><tr><td>5</td><td>&quot;2&quot;</td></tr><tr><td>6</td><td>&quot;2&quot;</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nNegative conditions also work. \n\n::: {.cell}\n\n```{.python .cell-code}\ndf.select(~cs.string())\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>b</th><th>c</th></tr><tr><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>3</td><td>7</td></tr><tr><td>4</td><td>8</td></tr><tr><td>5</td><td>9</td></tr><tr><td>6</td><td>0</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n#### In `with_columns`\n\nColumn selectors can play multiple rows in the transformation context.\n\nThe same transformation can be applied to multiple columns. Below, we find all integer variables, call a method to add 1 to each, and use the `name.suffix()` method to dynamically generate descriptive column names.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.with_columns(\n  cs.integer().add(1).name.suffix(\"_plus1\")\n)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th><th>c</th><th>b_plus1</th><th>c_plus1</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>3</td><td>7</td><td>4</td><td>8</td></tr><tr><td>&quot;1&quot;</td><td>4</td><td>8</td><td>5</td><td>9</td></tr><tr><td>&quot;2&quot;</td><td>5</td><td>9</td><td>6</td><td>10</td></tr><tr><td>&quot;2&quot;</td><td>6</td><td>0</td><td>7</td><td>1</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nWe can also use selected variables within transformations, like the rowwise sums that we just saw earlier.\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.with_columns(\n  row_total = pl.sum_horizontal(cs.integer())\n)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th><th>c</th><th>row_total</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>3</td><td>7</td><td>10</td></tr><tr><td>&quot;1&quot;</td><td>4</td><td>8</td><td>12</td></tr><tr><td>&quot;2&quot;</td><td>5</td><td>9</td><td>14</td></tr><tr><td>&quot;2&quot;</td><td>6</td><td>0</td><td>6</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n#### In `group_by` and `agg`\n\nColumn selectors can also be passed as inputs anywhere else that one or more columns is accepted, as with data aggregation.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.group_by(cs.string()).agg(cs.integer().sum())\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (2, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th><th>c</th></tr><tr><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>7</td><td>15</td></tr><tr><td>&quot;2&quot;</td><td>11</td><td>9</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### Consistent API for Window Functions\n\nWindow functions are another incredibly important tool in any data wrangling language but seem criminally undertaught in introductory analysis classes. Window functions allows you to apply aggregation *logic* over subgroups of data while preserving the original *grain* of the data (e.g. in a table of all customers and orders and a column for the max purchase account by customer). \n\n`dplyr` make window functions trivially easy with the `group_by()` + `mutate()` pattern, invoking users' pre-existing understanding of how to write aggregation logic and how to invoke transformations that preserve a table's grain. \n\n`polars` takes a slightly different but elegant approach. Similarly, it reuses the core `with_columns()` method for window functions. However, it uses a more SQL-reminiscent specification of the \"window\" in the column definition versus a separate grouping statement. This has the added advantage of allowing one to use multiple window functions with different windows in the same `with_columns()` call if you should so choose. \n\nA simple window function tranformation can be done by calling `with_columns()`, chaining an aggregation method onto a column, and following with the `over()` method to define the window of interest.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.with_columns(\n  min_b = pl.col('b').min().over('a')\n)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th><th>c</th><th>min_b</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>3</td><td>7</td><td>3</td></tr><tr><td>&quot;1&quot;</td><td>4</td><td>8</td><td>3</td></tr><tr><td>&quot;2&quot;</td><td>5</td><td>9</td><td>5</td></tr><tr><td>&quot;2&quot;</td><td>6</td><td>0</td><td>5</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThe chaining over and aggregate and `over()` can follow any other arbitrarily complex logic. Here, it follows a basic \"case when\"-type statement that creates an indicator for whether column b is null.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.with_columns(\n  n_b_odd = pl.when( (pl.col('b') % 2) == 0)\n              .then(1)\n              .otherwise(0)\n              .sum().over('a')\n)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th><th>c</th><th>n_b_odd</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i32</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>3</td><td>7</td><td>1</td></tr><tr><td>&quot;1&quot;</td><td>4</td><td>8</td><td>1</td></tr><tr><td>&quot;2&quot;</td><td>5</td><td>9</td><td>1</td></tr><tr><td>&quot;2&quot;</td><td>6</td><td>0</td><td>1</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### List Columns and Nested Frames\n\nWhile the R `tidyverse`'s raison d'etre was originally around the design of heavily normalize [tidy data](https://vita.had.co.nz/papers/tidy-data.pdf), modern data and analysis sometimes benefits from more complex and hierarchical data structures. Sometimes data comes to us in nested forms, like from an API^[For example, an API payload for a LinkedIn user might have nested data structures representing professional experience and educational experience], and other times nesting data can help us perform analysis more effectively^[For example, training a model on different data subsets.] Recognizing these use cases, `tidyr` provides many capability for the creation and manipulation of [nested data](https://tidyr.tidyverse.org/articles/nest.html) in which a single cell contains values from multiple columns or sometimes even a whoel miniature dataframe. \n\n`polars` makes these operations similarly easy with its own version of structs (list columns) and arrays (nested dataframes).\n\n#### List Columns & Nested Frames\n\nList columns that contain multiple key-value pairs (e.g. column-value) in a single column can be created with `pl.struct()` similar to R's `list()`. \n\n::: {.cell}\n\n```{.python .cell-code}\ndf.with_columns(list_col = pl.struct( cs.integer() ))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th><th>c</th><th>list_col</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>struct[2]</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>3</td><td>7</td><td>{3,7}</td></tr><tr><td>&quot;1&quot;</td><td>4</td><td>8</td><td>{4,8}</td></tr><tr><td>&quot;2&quot;</td><td>5</td><td>9</td><td>{5,9}</td></tr><tr><td>&quot;2&quot;</td><td>6</td><td>0</td><td>{6,0}</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThese structs can be further be aggregated across rows into miniature datasets.\n\n::: {.cell}\n\n```{.python .cell-code}\ndf.group_by('a').agg(list_col = pl.struct( cs.integer() ) )\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>list_col</th></tr><tr><td>str</td><td>list[struct[2]]</td></tr></thead><tbody><tr><td>&quot;2&quot;</td><td>[{5,9}, {6,0}]</td></tr><tr><td>&quot;1&quot;</td><td>[{3,7}, {4,8}]</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nIn fact, this could be a good use case for our column selectors! If we have many columns we want to keep unnested and many we want to next, it could be efficient to list out only the grouping variables and create our nested dataset by examining matches. \n\n\n::: {.cell}\n\n```{.python .cell-code}\ncols = ['a']\n(df\n  .group_by(cs.by_name(cols))\n  .agg(list_col = pl.struct(~cs.by_name(cols)))\n)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>list_col</th></tr><tr><td>str</td><td>list[struct[2]]</td></tr></thead><tbody><tr><td>&quot;2&quot;</td><td>[{5,9}, {6,0}]</td></tr><tr><td>&quot;1&quot;</td><td>[{3,7}, {4,8}]</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n#### Undoing\n\nJust as we constructed our nested data, we can denormalize it and return it to the original state in two steps. To see this, we can assign the nested structure above as `df_nested`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_nested = df.group_by('a').agg(list_col = pl.struct( cs.integer() ) )\n```\n:::\n\n\nFirst `explode()` returns the table to the original grain, leaving use with a single struct in each row.\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_nested.explode('list_col')\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>list_col</th></tr><tr><td>str</td><td>struct[2]</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>{3,7}</td></tr><tr><td>&quot;1&quot;</td><td>{4,8}</td></tr><tr><td>&quot;2&quot;</td><td>{5,9}</td></tr><tr><td>&quot;2&quot;</td><td>{6,0}</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThen, `unnest()` unpacks each struct and turns each element back into a column.\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_nested.explode('list_col').unnest('list_col')\n```\n\n::: {.cell-output-display}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th><th>c</th></tr><tr><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>3</td><td>7</td></tr><tr><td>&quot;1&quot;</td><td>4</td><td>8</td></tr><tr><td>&quot;2&quot;</td><td>5</td><td>9</td></tr><tr><td>&quot;2&quot;</td><td>6</td><td>0</td></tr></tbody></table></div>\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}