{
  "hash": "97df51787292639ddbab3dd50e62c5b1",
  "result": {
    "markdown": "---\ntitle: \"A Tale of Six States: Flexible data extraction with scraping and browser automation\"\ndescription: \"Exploring how `Playwright`'s headless browser automation (and its friends) can help unite the states' data\"\nauthor: \"Emily Riederer\"\ndate: \"2021-05-08\"\ncategories: [data, elt, python]\nimage: \"featured.png\"\naliases:\n  - /post/states-scraping-automation/\n---\n\n\n\n\n\nLike many Americans, last fall I was captivated by [Professor Michael McDonald](https://twitter.com/ElectProject)'s [US Elections Project](http://www.electproject.org/) and, in particular, his daily reporting of [early vote totals](https://electproject.github.io/Early-Vote-2020G/) across all 50 states. Not only did this reporting provide fuel for anxious speculation and satiate an innate desire for reading the tea leaves, but it was also a quite a feat in data management. \n\nIn many discussions of [modern data management](https://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/), the extract-load-transform (ELT) process is offered as a solution to data silos. However, these tools largely focus on abstracting away the different REST APIs of common sources (e.g. Salesforce, Google Analytics, Facebook Ads) and destinations (e.g. BigQuery, Redshift, Snowflake). At the same time, many data science resources introduce static web page scraping as a tool in one's toolkit but discuss less scraping websites rendered with client side JavaScript (as quite a bit of the internet is) or navigating arbitrary applications with browser automation.\n\nFor more \"creative\", persay, sources, we need to build our own solutions. And this is particularly true when we're attempting to access, standardize, and analyze 50 separate data sets published at different cadences, provided with different levels of granularity, partitioned along different dimensions, controlled by different levels of access and permission, and embedded in systems with vastly different underlying architecture. (Oh, and in Dr. McDonald's case, with a good number of Twitter users starting to tweet at you if you haven't refreshed your website by 8:02AM!)\n\nTo give an example of some of the diverse data sources used in the Elections Project:\n\n- [IL](https://elections.il.gov/VotingAndRegistrationSystems/PreElectionCounts.aspx?MID=l0hlXuSrKL0%3d&T=637317257975604269), [WI](https://elections.wi.gov/index.php/publications/statistics/absentee), and [NC](https://dl.ncsbe.gov/?prefix=ENRS/2020_11_03/) provide direct `csv` downloads with predictable URLs which contain all data to-date\n- [WV](http://services.sos.wv.gov/Elections/AbsenteeBallotStats) and [AK](https://www.elections.alaska.gov/doc/info/statstable.php) display data in static tables rendere by server-side PHP\n- [VA](https://www.vpap.org/elections/early-voting/) shows its data in a dashboard rendered by client-side JavaScript (which may superficially sound like the last bullet but has different implications for the types of scraping tools that could lend a hand^[Traditional tools like python's `BeautifulSoup` or R's `rvest` don't play nicely with dynamic websites where client-side JavaScript is modifying the DOM])\n- [GA](https://elections.sos.ga.gov/Elections/voterabsenteefile.do) and [TX](https://earlyvoting.texas-election.com/Elections/getElectionEVDates.do) offer downloadable data which requires navigating through a Java serverlet UI^[Or so [this StackExchange](https://stackoverflow.com/questions/3597582/why-do-java-webapps-use-do-extension-where-did-it-come-from) would suggest based on the `.do` extension] to access \n- [RI](https://app.powerbigov.us/view?r=eyJrIjoiMGEwN2E0MzUtOTA0OC00ZDA3LThjMTItZDZhYTBjYjU5ZjhjIiwidCI6IjJkMGYxZGI2LWRkNTktNDc3Mi04NjVmLTE5MTQxNzVkMDdjMiJ9) publishes data in a PowerBI dashboard\n- AL, AZ, MI, IA, and PA were among those that didn't publish data publicly and required Professor McDonald to directly contact their Secretary of State offices\n\nThe disparities in these data publishing formats and access patterns seemed like an interesting opportunity to compare different tools for data extraction. This posts starts out with a few trivial examples of using `requests` for HTTP requests and `BeautifulSoup` for static web scraping in order to understand the strengths and limitations for each tool and the unmet needs that browser automation helps address. We'll then switch over to `Playwright` (with a touch of `pytesseract`) for a sampling of the types of challenges that browser automation can tackle.\n\n- [NC: Direct Download with `urllib.request`](#nc-direct-download)\n- [WI: Direct Download with a Dynamic URL](#wi-direct-download-with-dynamic-url)\n- [AK: Web scraping with `BeautifulSoup`](#ak-web-scraping-with-beautifulsoup)\n- [VA: Web scraping with `Playwright`](#va-web-scraping-with-playwright)\n- [TX: Browser automation with `Playwright`](#tx-browser-automation-with-playwright)\n- [RI: Arbitrary UIs with OCR](#ri-arbitrary-uis-with-ocr)\n\n## NC: Direct Download\n\n![](nc.PNG)\n\nNorth Carolina provides one of the simplest paths to accessing paths to downloading its data. Data files live in a public S3 bucket with static file names based on the election date. Granular voter-level data can be downloaded easily with the `urllib.request` library.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport urllib.request\n\nurl = 'https://s3.amazonaws.com/dl.ncsbe.gov/ENRS/2020_11_03/absentee_counts_county_20201103.csv'\nurllib.request.urlretrieve(url, 'nc.csv')\n```\n:::\n\n\n## WI: Direct Download with Dynamic URL\n\n![](wi.PNG)\n\nWisconsin has similarly accessible data files available for download. However, when they update files during an election cycle, each new file is named by publishing date. So, unlike North Carolina, the URL of interest varies and it's not altogether obvious what the most current one is. \n\nWe can still use `requests` to download this data, but it requires more caution in constructing the URL. The `retrieve_date()` function requires an ISO 8601 date to be passed in and attempts to construct a URL from it. Our `GET` request returns a status code of 500 if no such path exists, at which point we can throw an exception. Some calling program could decrement the date and try again.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport requests\nimport datetime\n\ndef retrieve_date(date): \n\n  # format dates as needed\n  dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n  yyyy_mm = dt.strftime('%Y-%m')\n  mm_d_yyyy = dt.strftime('%m-%#d-%Y')\n  \n  # download csv\n  url = f'https://elections.wi.gov/sites/elections.wi.gov/files/{yyyy_mm}/AbsenteeCounts_County%20{mm_d_yyyy}.csv'\n  req = requests.get(url)\n  if req.status_code == 500:\n    raise Exception('Resource not found')\n  content = req.content\n  \n  # write to file\n  csv = open('wi.csv', 'wb')\n  csv.write(content)\n  csv.close()\n```\n:::\n\n\n## AK: Web Scraping with `BeautifulSoup`\n\n![](ak.PNG)\n\nInstead of provided a direct download, Arkansas publishes data to its election website by rendering a static HTML table with server-side PHP. As with before, we can retrieve this content with `requests`, but now we need to parse the output ourselves. Specifically, we want to iterate over table rows such as this:\n\n```\n<tr>\n  <td>Online Delivery</td>\n  <td>16,446</td>\n  <td>12,026</td>\n</tr>\n```\n\nFor this, we can use `BeautifulSoup` to iterate through the table and save the results to a CSV file with `pandas`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom bs4 import BeautifulSoup\nimport requests\nimport datetime\nimport pandas as pd\n\nurl = \"https://www.elections.alaska.gov/doc/info/statstable.php\"\nhtml_content = requests.get(url).text\nsoup = BeautifulSoup(html_content, \"lxml\")\ntable = soup.find(\"table\", attrs = {\"class\":\"lctbl\"})\n\ndata = []\nrows = table.tbody.find_all(\"tr\")\n\n# iterate over rows excluding first (header) and last (total)\nfor r in range(1, len(rows) - 1): \n\n  row = rows[r]\n  vals = [d.get_text() for d in row.find_all(\"td\")]\n  # convert count columns to numeric\n  vals[1] = int(vals[1].replace(',',''))\n  vals[2] = int(vals[2].replace(',',''))\n    \n  data.append(vals)\n  \n# save resulting data\ndf = pd.DataFrame(data, columns = ['channel','n_issued','n_returned'])\ndf['dt_updated'] = dt_fmt\ndf.to_csv('ak.csv', index = False)\n```\n:::\n\n\n## VA: Web Scraping with `Playwright`\n\n![](va.PNG)\n\nVirginia seems superficially similar to Arkansas insomuch as data is provided in an in-browser display. However, since this display is constructed with browser-side JavaScript, it won't appear in the content that results from a call to `requests.get()`. \n\nMany different approaches exist to force JavaScript to update the DOM before accessing the source^[Selenium and PhantomJS are popular related tools]. Arguably Microsoft's `Playwright` is overly complicated for this situation, but I use it for this example since it is a flexible tool for browser automation, and the next three examples help demonstrate the variety of features it offers.\n\nThe following script navigates to Virginia's website with a headless Firefox browser and then extracts vote counts stored as attributes in the bar chart tooltip. Each bar is create with script like this: \n\n```\n<rect x=\"95.333\" y=\"101\" width=\"7.333\" height=\"104\" \n      data-toggle=\"popever\" data-placement=\"top\" title \n      data-content=\"In-Person: 140<br />Mail: 94<br />Total: 234\" \n      data-original-title=\"Sep 30\">\n</rect>\n```\n\nAs before, results are then coerced into a `pandas` dataframe and written to a `csv`.\n\nThe following script defines a `retrieve_county()` function to parse out this information for a single county.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom playwright.sync_api import sync_playwright\nimport datetime\nimport re\nimport pandas as pd\n\ndef retrieve_county(county, page):\n\n  # navigate to county-specific page\n  county_url = county.lower().replace(' ','-')\n  page.goto(f'https://www.vpap.org/elections/early-voting/year-2020/{county_url}-va')\n  \n  county_records = []\n  \n  for n in range(1,100):\n  \n    selector = f'#timeline g.popovers rect:nth-of-type({n})'\n    try:\n      date = page.get_attribute(selector, 'data-original-title')\n      vals = page.get_attribute(selector, 'data-content')\n    except:\n      break\n\n    # process data into tabular structure\n    vals_method = re.search('In-Person: (\\d+)<br />Mail: (\\d+)<br />Total: (\\d+)', vals.replace(',',''))\n    date_parse = datetime.datetime.strptime(date + ' 2020', '%b %d %Y').strftime('%Y-%m-%d')\n    county_records.append([county, date_parse, 'In-Person', vals_method.group(1)])\n    county_records.append([county, date_parse, 'Mail', vals_method.group(2)])\n    \n  return county_records\n```\n:::\n\n\nAs a low fidelity but transparent example of it in operation, I show it looping over a few counties. In reality, we'd add more exception handling or save interim results separately so failures in any one county did not take down the whole process.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwith sync_playwright() as p:\n\n  # set up\n  browser = p.firefox.launch()\n  context = browser.new_context(accept_downloads = True)\n  page = context.new_page()\n  \n  # iterate over counties\n  county = ['Accomack County', 'Albemarle County', 'Alexandria City']\n  records = []\n  for c in county:\n    records += retrieve_county(c, page)\n  \n  # save resulting data\n  df = pd.DataFrame(records, columns = ['county', 'date', 'channel', 'n'])\n  df.to_csv('va.csv', index = False)\n\n  # cleanup\n  page.close()\n  context.close()\n  browser.close()\n```\n:::\n\n\nThis creates the following data structure:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  county          date       channel       n\n  <chr>           <date>     <chr>     <dbl>\n1 Accomack County 2020-09-17 In-Person     0\n2 Accomack County 2020-09-17 Mail          0\n3 Accomack County 2020-09-18 In-Person   212\n4 Accomack County 2020-09-18 Mail          0\n5 Accomack County 2020-09-19 In-Person     0\n6 Accomack County 2020-09-19 Mail          0\n```\n:::\n:::\n\n\n\n## TX: Browser Automation with `Playwright`\n\n![](tx.gif)\n\nNext up, we come to Texas. Texas, somewhat paradoxically, generously provides rich data (by day, by voting method, by county, and even *by person*) and yet does so in a way that is particularly tedious to access. Navigating to the data download requires selecting an election and then a voting date out of a UI before clicking a button in a Java serverlet^[The significance of this is that this button is not like clicking a link with a specific web URL.] that triggers the creation and downloading of a report as shown above.\n\nThis is where `Playwright` really shines. As with Virginia, it loads the Texas' data in a headless^[Headless basically just means the browser doesn't physically open on our computer so we don't see it. Time and compute resources aren't wasted on \"painting\" the browser to show us what is happening. However, if you enjoy watching your computer work in a \"Look Mom, no hands!\" sort of way, you can use the `headless = False` option when launching the browser.] browser. But beyond just opening a browser, `Playwright` can interact with it in the same way as a user: selecting options from menus, clicking buttons, and more.\n\nIn the `retrieve_date()` function below, I tell my browser exactly what I want it to do: go to the website, pick an election, click submit, pick a date, click submit, and then finally click a button to download data.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom playwright.sync_api import sync_playwright\nimport datetime\n\ndef retrieve_date(date, page):\n\n  # navigate to date-specific page \n  target_date = datetime.datetime.strptime(date, '%Y%m%d')\n  target_date_str = target_date.strftime('%Y-%m-%d 00:00:00.0')\n  target_file = 'tx-' + target_date.strftime('%Y%m%d') + '.csv'\n  \n  # pick election\n  page.goto('https://earlyvoting.texas-election.com/Elections/getElectionDetails.do')\n  page.select_option('#idElection', label = \"2020 NOVEMBER 3RD GENERAL ELECTION\")\n  page.click('#electionsInfoForm button')\n  page.wait_for_selector('#selectedDate')\n  \n  # pick day\n  page.select_option('#selectedDate', value = target_date_str)\n  page.click('#electionsInfoForm button:nth-child(2)')\n  page.wait_for_selector('\"Generate Statewide Report\"')\n\n  # download report  \n  with page.expect_download() as download_info:\n    page.click('\"Generate Statewide Report\"')\n  download = download_info.value\n  download.save_as(target_file)\n```\n:::\n\n\nThis function could then be called for one or more dates of interest:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwith sync_playwright() as p:\n\n  browser = p.firefox.launch()\n  context = browser.new_context(accept_downloads = True)\n  page = context.new_page()\n  \n  dates = ['20201020','20201021','20201022']\n  for d in dates:\n    retrieve_date(d, page)\n\n  # cleanup\n  page.close()\n  context.close()\n  browser.close()\n```\n:::\n\n\n## RI: Arbitrary UIs with OCR\n\n![](ri.png)\n\nThe ability to navigate around a UI starts to blend the capabilities of `Playwright`'s *browser automation* with the more full-fledged concept of robotic process automation (RPA). RPA tools can similarly navigate arbitrary non-browser-based UIs to perform manual tasks with great speed. Abritrary UIs lack many of the features we've been using so far such as Xpaths and CSS IDs and classes to tell our tools where to do what. Instead, their often have built-in optical character recognition (OCR) to recognize buttons or input boxes \"on sight\". \n\n`Playwright` doesn't quite have these capabilities built in natively, but it does offer users the ability to screenshot their browser. This allows us to pass the screenshot to `pytesseract` for OCR in a similar manner. \n\nThis techniques comes in handy for Rhode Island whose data is hosted in an embedded PowerBI app. The following script navigates to and screenshots the app, converts the resulting image to text, extracts the total vote count, and writes the results to PDFs.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport cv2\nimport pytesseract\nfrom playwright.sync_api import sync_playwright\nimport time\nimport re\nimport pandas as pd\npytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n\nwith sync_playwright() as p:\n\n  # set up\n  browser = p.firefox.launch()\n  context = browser.new_context(accept_downloads = True)\n  page = context.new_page()\n  page.goto('https://app.powerbigov.us/view?r=eyJrIjoiMGEwN2E0MzUtOTA0OC00ZDA3LThjMTItZDZhYTBjYjU5ZjhjIiwidCI6IjJkMGYxZGI2LWRkNTktNDc3Mi04NjVmLTE5MTQxNzVkMDdjMiJ9')\n  page.wait_for_load_state(state = 'networkidle')\n  time.sleep(30)\n  page.screenshot(path = 'ri.png')\n  \n  # cleanup\n  page.close()\n  context.close()\n  browser.close()\n\n# extract text\nimg = cv2.imread('ri.png')\ntext = pytesseract.image_to_string(img)\nn_tot = re.search('Turnout\\n\\n(\\d+)', text.replace(',','')).group(1)\nn_mail = re.search('Mail Ballots Received by BOE\\n\\n(\\d+)', text.replace(',','')).group(1)\n\n# write output\ndf = pd.DataFrame([[n_tot, n_mail]], columns = ['n_tot','n_mail'])\ndf.to_csv('ri.csv', index = False)\n```\n:::\n\n\nThis creates the following data structure:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n   n_tot n_mail\n   <dbl>  <dbl>\n1 305724 156178\n```\n:::\n:::\n\n\n\n## What Next?\n\nSimply *accessing* data is only the first in many steps towards unifying and analyzing it. The full scope of the US Elections Project requires far more numerous and challenging steps including:\n\n- Understanding the exact schema and variables of each data set\n- Understanding when historical data may be modified or corrected and adjusting accordingly\n- Accounting for sudden changes to reporting formats, cadences, or locations\n- Obtaining data from states where the `robots.txt` prevents scraping (I'm looking at you, IL)\n- Building relationships with Secretary of States' offices where data is not publicly available\n\nRegardless, surveying what different states choose to publish and how they choose to share it provides an interesting opportunity to think about data access, usability, and available technologies for data retrieval.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}