---
output: hugodown::md_document
title: "Workflows for querying databases via R"
subtitle: "Tricks for modularizing and refactoring your projects SQL/R interface"
summary: ""
authors: []
tags: [rstats, workflow, sql]
categories: [rstats, workflow, sql]
date: 2021-07-17
lastmod: 2021-07-17
featured: false
draft: false
aliases:

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: true

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: [""]
---

Simple, self-contained, reproducible examples are a common part of good software documentation. However, in the spirit of brevity, these examples often do not demonstrate the most sustainable or flexible *workflows* for integrating software tools into large projects. In this post, I document a few mundane but useful patterns for querying databases in R using the `DBI` package. 

A prototypical example of forming and using a database connection with `DBI` might look something like this:


```{r}
library(DBI)

con <- dbConnect(RSQLite::SQLite(), ":memory:")
dbWriteTable(con, "diamonds", ggplot2::diamonds)
dat <- dbGetQuery(con, "select cut, count(*) as n from diamonds group by 1")
dat
```

A connection is formed (in this case to a fake database that lives only in my computer's RAM), the `diamonds` dataset from the `ggplot2` package is written to the database (once again, this is for example purposes only; a real database would, of course, have data), and `dbGetQuery()` executes a querying on the resulting table. 

However, as queries get longer and more complex, this succinct solution becomes less attractive. Writing the query directly inside `dbGetQuery()` blurs the line between "glue code" (rote connection and execution) and our more nuanced, problem-specific logic. This makes the latter harder to extract, share, and version. 

Below, I demonstrate a few alternatives that I find helpful in different circumstances such as reading queries that are saved separately (in different files or at web URLs) and forming increasingly complex query templates.

## Read query from separate file

A first enhancement is to isolate your SQL script in a separate file than the "glue code" that executes it. This improves readability and makes scripts more portable between projects. If a coworker who uses python or runs SQL through some other tool wishes to use your script, it's more obvious which parts are relevant. Additionally, its easier to version control: we likely care far more about changes to the actual query than the boilerplate code that executes it so it feels more transparent to track them separately.

To do this, we can save our query in a separate file. We'll call it `query-cut.sql`:

```{r, code=readLines("query-cut.sql"), eval = FALSE}
```

Then, in our script that pulls the data, we can read that other file with `readLines()` and give the results of that to the `dbGetQuery()` function.

```{r echo = -4L}
library(DBI)

con <- dbConnect(RSQLite::SQLite(), ":memory:")
dbWriteTable(con, "diamonds", ggplot2::diamonds)
query <- paste(readLines("query-cut.sql"), collapse = "\n")
dat <- dbGetQuery(con, query)
dat
```

Of course, if you wish you could define a helper function for the bulky `paste(readLines(...))` bit.

```{r}
read_source <- function(path) {paste(readLines(path), collapse = "\n")}
```

## Read query from URL (like GitHub)

Sometimes, you might prefer that your query not live in your project at all. For example, if a query is used across multiple projects or if it changes frequently or is maintained by multiple people, it might live in a separate repository. In this case, the exact same workflow may be used if the path is replaced by a URL to a plain-text version of the query. (On GitHub, you may find such a link by clicking the "Raw" button when a file is pulled up.)

```{r echo = -4L, warning = FALSE}
library(DBI)

con <- dbConnect(RSQLite::SQLite(), ":memory:")
dbWriteTable(con, "diamonds", ggplot2::diamonds)
url <- "https://raw.githubusercontent.com/emilyriederer/dbtplyr/main/macros/across.sql"
query <- paste(readLines(url), collapse = "\n")
#dat <- dbGetQuery(con, query)
dat
```

This works because the `query` variable simply contains our complete text file read from the internet:

```{r}
cat(query)
```

Alternatively, in an institutional setting you may find that you need some sort of authentication or proxy to access GitHub from R. In that case, you may retrieve the same query with an HTTP request instead using the `httr` package.

```{r}
library(httr)

url <- "https://raw.githubusercontent.com/emilyriederer/dbtplyr/main/macros/across.sql"
query <- content(GET(url))
cat(query)
```

## Use query template

Separating the query from its actual execution also allows us to do query pre-processing. For example, instead of a normal query, we could write a query template with a wildcard variable. Consider the file `template-cut.sql`:

```{r, code=readLines("template-cut.sql"), eval = FALSE}
```

This query continues to count the number of diamonds in our dataset by their cut classification, but now is has parameterized the `max_price` variable. Then, we may use the `glue` package to populate this template with a value of interest before executing the script.

```{r echo = -5L, warning = FALSE}
library(DBI)
library(glue)

con <- dbConnect(RSQLite::SQLite(), ":memory:")
dbWriteTable(con, "diamonds", ggplot2::diamonds)
template <- paste(readLines("template-cut.sql"), collapse = "\n")
query <- glue(template, max_price = 500)
#dat <- dbGetQuery(con, query)
dat
```

This is a useful alternative to databases that do not allow for local variables. 

## Compose more complex queries

The idea of templating opens up far more interesting possibilities. For example, consider a case where you wish to frequently create the same data structure for a different population of observations (e.g. a standard set of KPIs for different A/B test experiments, reporting for different business units, etc.) 

A boilerplate part of the query could be defined as a template ready to accept a CTE or a subquery for a specific population of interest. For example, we could write a file `template-multi.sql`:

```{r, code=readLines("template-multi.sql"), eval = FALSE}
```

Then our "glue code" can combine the static and dynamic parts of the query at runtime before executing. 

```{r}
template <- paste(readLines("template-multi.sql"), collapse = "\n")
query_sample <- "select * from diamonds where cut = 'Very Good' and carat < 0.25"
query <- glue(template, query_sample = query_sample)
cat(query)
```

(Of course, this may seem like overkill and an unnecessarily inefficent query for the example above where a few more `where` conditions could have sufficed. But one can imagine more useful applications in a traditional setting where multiple tables are being joined.)

## Query package

Finally, these queries and query templates could even be shipped as part of an R package. Additional text files may be placed in the `inst/` directory and their paths discovered by `system.file()`. So, if your package `myPkg` were to contain the `template-multi.sql` file we saw above, you could provide a function to access it like so:

```{r}
construct_query <- function(stub, ...) {
  
  path <- system.file(stub, package = 'myPkg')
  template <- paste(readLines(path), collapse = '\n')
  query <- glue::glue(template, ...)
  return(query)
  
}
```

Then, that function could be called like so:

```{r eval = FALSE}
sample <- "select * from diamonds where cut = 'Very Good' and carat < 0.25"
query <- construct_query("multi", query_sample = sample)
```

This approach has some benefits such as making it easier to share queries across users and benefit from package versioning and environment management standards. However, there are of course other risks; only dynamically generating queries could limit reproducibility or documentation about the actual query run to generate data. Thus, it might be a good idea to save the resulting query along with the resulting data:

```{r eval = FALSE}
dat <- dbGetQuery(con, query)
saveRDS(dat, "data.rds")
writeLines(query, "query-data.rds")
```



