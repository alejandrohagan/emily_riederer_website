---
output: hugodown::md_document
title: "Make data brands not data products"
subtitle: ""
summary: "There's no data quality without data fit, but applying the jobs-to-be-done framework can help us do better than a 'one-size-fits-some' solution. With examples from New York, Chicago, and San Francisco open transit data."
authors: []
tags: [data, jtbd]
categories: [data, jtbd]
date: 2020-12-27
lastmod: 2020-12-27
featured: false
draft: true
aliases:

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: true

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: [""]
---







## Jobs to be Done

To motivate the design goals and decisions we'll discuss, it's useful to think about Harvard Business School Professor Clayton Christensen's [jobs-to-be-done](https://hbr.org/2016/09/know-your-customers-jobs-to-be-done) theory of disruptive innovation. This asserts that

> we *hire* a product to do a job that helps us make progress towards a goal

In the world of product development, that focus on the *progress* a customer is a critical distinction from the conventional understanding of competition and industry makeup. For example, if I ask you who Twitter's competitors are, you might first think of Facebook, Instagram, or TikTok -- other entrants in the category of mobile apps for social networking. However, if we think about the jobs that I might "hire" Twitter to do, I might consider "hiring" Twitter to help me pass the time while I wait in a long line or (back in "the olden days") ride my long commute to work. Through that lens, it not longer matters what industry Twitter is nominally in; this is about me and my needs - not Twitter. So, Twitter is "competing" for the position of my travel companion against all sorts of other potential hires like Spotify, podcast apps, books, or even an extra 20 minutes of sleep on the train.

## New York City (Raw Data)

The [turnstile data published by the New York MTA](http://web.mta.info/developers/turnstile.html) is published weekly as a CSV file with the following fields:

```{r echo = FALSE, message = FALSE}
library(readr)
data_nyc <- read_csv("http://web.mta.info/developers/data/nyct/turnstile/turnstile_201219.txt")
head(data_nyc)
```


`C/A,UNIT,SCP,STATION,LINENAME,DIVISION,DATE,TIME,DESC,ENTRIES,EXITS`

## Chicago (Cleaned)

## San Francisco (Pre-Baked Answers)




Bringing this back to packages, open source packages know a *task* we want done (e.g. "fit a Cox proportional hazard model"), but what really sets internal packages apart is that they can use their knowledge of our organization to cater to the kind of *progress* that we truly want to make. It's hard for passionate R users to imagine, but no one actually *wants* a package. I'd even go as far to say that no one even *wants* data (blasphemy!) Organizations need *progress* -- they need strategies, fueled by decisions, fueled by answers to questions, and fueled, in turn and in part, by data sets and tools. 




 




Google's popular [COVID-19 Community Mobility Reports](https://www.google.com/covid19/mobility/) inspired me to explore more granular sources of transportation data in major US cities^[One small piece of exploratory analysis leading to the map in this posts cover photo is available in this gist: https://gist.github.com/emilyriederer/79e08b1bdfdf75094c01743bda68b930]. Exploring the open data portals for Chicago, New York, and San Francisco, it quickly became eviden




explore more granular mobility trends using Chicago's [open data](https://data.cityofchicago.org/) on public transit. After some preliminary [exploration](https://gist.github.com/emilyriederer/79e08b1bdfdf75094c01743bda68b930), I became interested in comparing trends across major cities that vary in how commercial and residential areas are arranged, how income is distributed, and of course how different the stay-at-home policies and messaging were.

significant differences in how commercial and residential 




Early on in the pandemic, Google's [COVID-19 Community Mobility Reports](https://www.google.com/covid19/mobility/) provided 


## Intro / Motivation

- Making a plot like this: https://gist.github.com/emilyriederer/79e08b1bdfdf75094c01743bda68b930
- Angela Bassa's [Data Alone is not Ground Truth](https://medium.com/@angebassa/data-alone-isnt-ground-truth-9e733079dfd4)

## Chicago

- specific: https://data.cityofchicago.org/Transportation/CTA-Ridership-L-Station-Entries-Daily-Totals/5neh-572f
- also available: https://www.transitchicago.com/data/
- can retrieve thru API
- id | name | date | daytype (W=Weekday, A=Saturday, U=Sunday/Holiday) | rides (by station entry)
- daily
- readme: https://data.cityofchicago.org/api/assets/CCAFA078-D8FC-43EE-8FAA-28007103DA2E

> Daytype fields in the data are coded as "W" for Weekday, "A" for Saturday and "U" for Sunday/Holidays.  Note that New Year's Day, Memorial Day, Independence Day, Labor Day, Thanksgiving, and Christmas Day are considered as "Sundays" for the purposes of ridership reporting.  All other holidays are reported as the type of day they fall on.

> Customers are not counted as "entries" when they make a "cross-platform" transfer from one rail line to another, since they don't pass through a turnstile. Where the number given for rail is in "boardings," what's presented is a statistically valid estimate of the actual number of boardings onto the rail system. 

## New York

- specific: http://web.mta.info/developers/turnstile.html
- need to read individual weekly flat files
- C/A,UNIT,SCP,STATION,LINENAME,DIVISION,DATE,TIME,DESC,ENTRIES,EXITS
- every four hours
- minimal metadata: http://web.mta.info/developers/resources/nyct/turnstile/ts_Field_Description.txt

raw and messy but highest quality in the sense that it's exactly what sensors say 

treatments:
- mean imputation: http://www.columbia.edu/~yh2693/MTA_data.html
- setting arbitrary max: https://medium.com/qri-io/taming-the-mtas-unruly-turnstile-data-c945f5f96ba0
- dropping: 
  + http://transitdatatoolkit.com/lessons/subway-turnstile-data/
  + https://rjh336.github.io/projects/MTA-Turnstiles/
  + https://rstudio-pubs-static.s3.amazonaws.com/92744_51eb2f1ecce040caaea481928ed43d6f.html

## San Francisco

- many different formats but all very focused on out-of-box reporting
- source: https://www.bart.gov/about/reports/ridership
- more granular as an artifact of pricing system, as described [here](https://www.fastcompany.com/3026711/a-twitter-data-scientist-hacks-san-franciscos-subway-fares)

options:

- monthly ridership reports as origin/destination matrices; tabs for Avg Weekday, Avg Sat, Avg Sun, Monthly Overall
- daily x hourly (but which hour??) origin-destination total **exits** 
- compressed yearly files
- daily: 
  + single file going back to 90s. 1 row per day, 1 col per station
  + what is up with the quarter field?!
  + inconsistent holiday definition. Jan 1 1998 was holiday but Jan 1 1999 was not? Both NYE and NYD for 2000 are 
- annual:
  + like report with metrics like % change pre computed
  + asterisks in cells and excel comments to note things like strikes or new stations. yikes
- avg weekday exits
  + station names but no ids
  + stations as rows, FYs as cols
  + more granular data brings into question what a weekday really is
- data explorer w precanned views: https://data.bart.gov/dataset/ridership-average-total-exits-trend/resource/8eb9f17d-44a9-45d9-a8d5-93a5f6e55804
