---
output: hugodown::md_document
title: "Make grouping a first class citizen in data quality checks"
subtitle: ""
summary: "Advocating for a small tweak that could add a lot of value in emerging data quality tools"
authors: []
tags: [data]
categories: [data]
date: 2021-11-15
lastmod: 2021-11-15
featured: false
draft: false
aliases:

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: "Photo credit to [Greyson Joralemon](https://unsplash.com/@greysonjoralemon) on Unsplash"
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: [""]
---

The past few years have seen an explosion in different solutions for monitoring in-production data quality. These tools, including software like `dbt` and `Great Expectations` as well as platforms like `Monte Carlo`, bring a more DevOps flavor to data production with important functionality like automated testing *within* pipelines (not just at the end), expressive and configurable semantics for common data checks, and more.

However, despite all these features, I notice a common gap across the landscape which may limit the ability of these tools to detect common classes of data failures. Earlier this year, I wrote about the importance of [validating data based on its data generating process](https://emilyriederer.netlify.app/post/data-error-gen/) -- both along the technical and conceptual dimensions. Following this logic, an important and lacking functionality^[With the exception of `pointblank` which kindly entertained an issue I opened on this topic: https://github.com/rich-iannone/pointblank/issues/300] across the data quality monitoring landscape, is the ability to readily apply checks separately to groups of data. On a quick survey, I count about 
8/14 `dbt` tests (from the add-on [dbtutils](https://github.com/dbt-labs/dbt-utils) package), 
15/37 [Great Expectations](https://docs.greatexpectations.io/docs/reference/glossary_of_expectations) column tests, and 
most all of the [Monte Carlo](https://docs.getmontecarlo.com/docs/field-health-metrics) field health metrics that would be improved with first-class grouping functionality. (Lists at the bottom of the post.)

As we will see in the next section, group-based checks can be important for fully articulating good "business rules" against which to assess data quality. For example, groups could reflect either computationally-relevant dimensions of the ETL process (e.g. data loaded from different sources) or semantically-relevant dimensions of the real-world process that our data captures (e.g. repeated measures pertaining to many individual customers, patients, product lines, etc.)

In this post, I make a brief plea for why grouping should be a first-class citizen in data quality tooling.

## Use Cases

There are three main use-cases for enabling easy data quality checks by group: checks that can only be expressed by group, checks that can be more rigorous by group, and checks that are more semantically intuitive by group. 

**Some checks can be more rigorous by group.** Consider a recency check (i.e. that the maximum date represented in the data is appropriately close to the present.) If the data loads from multiple sources (e.g. customer acquisitons from web and mobile perhaps logging to different source systems), the maximum value of the field could pass the check if any one source loaded, but unless the data is grouped in such a way that reflects different data sources and *each* group's maximum date is checked, stale data could go undetected.

**Some types of checks can only be expressed by group.** Consider a check for consecutive data values. If a table that measures some sort of user engagements, there might be fields for the `USER_ID` and `MONTHS_SINCE_ACQUISITION`. A month counter will most certainly *not* be strictly increasing across the entire dataset but absolutely should be monotonic within each user.

**Some checks are more semantically intuitive by group.** Consider a uniqueness check for the same example as above. The month counter is also not unique across the whole table but could be checked for uniqueness within each user. Group semantics would not be required to accomplish this; a simple `USER_ID x MONTHS_SINCE_ACQUISITION` composite variable could be produced and checked for uniqueness. However, it feels cumbersome and less semantically meaningful to derive additional fields just to fully check the properties of existing fields.

## Alternatives Considered

Given that this post is, to some extent, a feature request across all data quality tools ever, it's only polite to discuss downsides and alternative solutions that I considered. Clearly, finer-grained checks incur a greater computational cost and could, in some cases, be achieved via other means. 

*Grouped checks are more computational expensive.* Partitioning and grouping can make data check operations more expensive by disabling certain computational shortcuts^[For example, the maximum of a set of numbers is the maximum of the maximums of the subsets. Thus, if my data is distributed, I can find the max by comparing only summary statistics from the distributed subsets instead of pulling all of the raw data back together.] and requiring more total data to be retained. This is particularly true if the data is indexed or partitioned along different dimensions than the groups used for checks. The extra time required to run more fine-grained checks could become intractable or at least unappealing, particularly in an interactive or continuous integration context. However, in many cases it could be a better use of time to more rigorously test recently loaded data as opposed to (or in conjunction with) running higher-level checks across larger swaths of data.

*Some grouped checks can be achieved in other ways.* This is really the same argument as the third point discussed above. Some (but not all) of these checks can be mocked by creating composite variables or, in the case of `Great Expectation`'s python-based API, writing custom code to partition data before applying checks^[This is less possible for tools like `dbt`/`dbt-utils` where tests are defined by SQL scripts. In this set-up, separate, similar testing macros would have to be defined.]. However, there solutions seem to defy part of the benefits of these tools: semantically meaningful checks wrapped in readable syntax and ready for use out-of-the-box. This also implies that grouped operations are far less than first-class citizens. This also limits the ability to make use of some of the excellent functionality these tools offer for documenting data quality checks in metadata and reporting on their outcomes.

## Survey of available tools

My goal is in no way to critique any of the amazing, feature-rich data quality tools available today. However, to further illustrate my point, I pulled down key data checks from a few prominent packages to assess how many of their tests would be potentially enhanced with the ability to provided grouping parameters. Below are lists for `dbt-utils`, `Great Expectations`, and `Monte Carlo` with relevant tests *in bold*. 

### dbt-utils (8 / 14)


- **equal_rowcount**
- equality
- expression_is_true 
- **recency**     
- **at_least_one**
- **not_constant**
- **cardinality_equality**
- **unique_where**
- not_null_where  
- **not_null_proportion**
- relationships_where
- mutually_exclusive_ranges
- **unique_combination_of_columns** (*but less important - only for semantics*)
- accepted_range

### Great Expectations (15 / 37)

- **expect_column_values_to_be_unique** (*but less important - only for semantics*)
- expect_column_values_to_not_be_null  
- expect_column_values_to_be_null    
- expect_column_values_to_be_of_type   
- expect_column_values_to_be_in_type_list
- expect_column_values_to_be_in_set
- expect_column_values_to_not_be_in_set
- expect_column_values_to_be_between      
- **expect_column_values_to_be_increasing**
- **expect_column_values_to_be_decreasing**
- expect_column_value_lengths_to_be_between
- expect_column_value_lengths_to_equal
- expect_column_values_to_match_regex
- expect_column_values_to_not_match_regex
- expect_column_values_to_match_regex_list
- expect_column_values_to_not_match_regex_list
- expect_column_values_to_match_like_pattern
- expect_column_values_to_not_match_like_pattern
- expect_column_values_to_match_like_pattern_list
- expect_column_values_to_not_match_like_pattern_list 
- expect_column_values_to_match_strftime_format
- expect_column_values_to_be_dateutil_parseable
- expect_column_values_to_be_json_parseable
- expect_column_values_to_match_json_schema
- expect_column_distinct_values_to_be_in_set
- **expect_column_distinct_values_to_contain_set**
- **expect_column_distinct_values_to_equal_set**
- **expect_column_mean_to_be_between**
- **expect_column_median_to_be_between**
- **expect_column_quantile_values_to_be_between**
- **expect_column_stdev_to_be_between**
- **expect_column_unique_value_count_to_be_between**
- **expect_column_proportion_of_unique_values_to_be_between**
- **expect_column_most_common_value_to_be_in_set**
- **expect_column_max_to_be_between**
- **expect_column_min_to_be_between**
- **expect_column_sum_to_be_between**

### Monte Carlo (All)

Any of Monte Carlo's checks might be more sensitive to detecting changes with subgrouping. Since these "health metrics" tend to represent distributional properties, it can be useful to ensure that "good groups" aren't pulling down the average value and masking errors in "bad groups".

- Pct NUll  
- Pct Unique
- Pct Zero  
- Pct Negative  
- Min 
- p20
- p40
- p60
- p80
- Mean
- Std
- Max
- Pct Whitespace
- Pct Integer
- Pct "Null"/"None"
- Pct Float
- Pct UUID
