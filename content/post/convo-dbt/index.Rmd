---
output: hugodown::md_document
title: "Embedding controlled vocabularies in data pipelines with dbt"
subtitle: ""
summary: "dbt supercharges SQL with Jinja templating, macros, and testing -- all of which can be customized to enforce controlled vocabularies and their implied contracts on a data model"
authors: []
tags: [data, sql]
categories: [data, sql]
date: 2021-02-06
lastmod: 2021-02-06
featured: false
draft: false
aliases:

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: "Data model DAG autogenerated by dbt"
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: [""]
---

In my post [Column Names as Contracts](/post/column-name-contracts/), I explore how using controlled vocabularies to name fields in a data table can create performance contracts between data producers and data consumers. I claim that field names can encode metadata and illustrate with R and python how these names can be used to improve data discoverability, wrangling, and validation. 

However, demonstrations with R and python are biased towards the needs of data consumers. These popular data analysis tools provide handy, high-level interfaces for programmatically operating on columns. For example, `dplyr`'s [select helpers](https://tidyselect.r-lib.org/reference/select_helpers.html) make it easy to quickly manipulate all columns whose names match given patterns. For example, suppose I know that all variables beginning with `IND_` are binary and non-null so I may sum them to get a count or average them to get a valid proportion. I can succinctly write:

```{r eval = FALSE}
summarize_at(my_data,
             .vars = vars(starts_with("IND")),
             .funs = list(sum, mean))
```

In contrast, SQL remains a mainstay for data production -- both for traditional relational databases and SQL interfaces for modern large-scale data processing engines like Spark. As a *very* high-level and declarative language, SQL variants generally don't offer a control flow (e.g. for loops, if statements) or programmatic control which would allow for similar operations on columns as shown above. That is, one might have to manually write:

```{r eval = FALSE}
select 
  mean(ind_a), 
  mean(ind_b), 
  sum(ind_a), 
  sum(ind_b)
from my_data
```

But that is tedious, static (would not automatically adapt to the addition of more indicator variables), and error-prone (easy to miss or mistype a variable). 

Although SQL itself is relatively inflexible, recent tools have added a layer of "programmability" on top of SQL which allows for far more flexibility and customization. In this post, I'll demonstrate how one such tool, `dbt` can help data producers consistently apply controlled vocabularies when defining, manipulating, and testing tables for analytical users.

## A brief intro to `dbt`

`dbt` (which stands for [Data Build Tools](https://www.getdbt.com/)) "applies the principles of software engineering to analytics code". Specifically, it encourages data producers to write modular, atomic SQL `select` statements in separate files (as opposed to the use of CTEs or subqueries) from which dbt derives a DAG and orchestrates the execution. Further, it enables the ability to write more programmatic (with control flow) SQL *templates* with `Jinja2` which `dbt` compiles to standard SQL files before executing. 

Specific advantages of this approach include:

- Templating with `if` statements and `for` loops
- Dynamic insertion of local variables^[Some but not all databases offer this natively]
- The ability to switch between dev and production schemas
- Easy toggling between views, tables, and inserts for the same base logic
- Automated testing of each modular SQL unit 
- Code sharing with tests and macros exportable in a package framework

Additional `dbt` features include:

- Automatic generation of a static website documenting data lineage, metadata, and test results (the featured image above is a screenshot from the created website)
- Orchestration of SQL statements in the DAG
- Hooks for rote database management tasks like adding indices and keys or granting access

For a general overview to `dbt`, check out the [introductory tutorial](https://docs.getdbt.com/tutorial/setting-up) on their website, the [dbt101 presentation](https://www.getdbt.com/coalesce/agenda/dbt-101-eu-and-us-friendly) from their recent Coalesce conference^[One excellent feature of this project is the impressive amount of onboarding and documentation materials], or the interview with one of their founders on the [Data Engineering Today](https://open.spotify.com/episode/1gKKgR8eZgdqdXztFGGkFe) podcast.

In this post, I'll demonstrate how three features of `dbt` can be used to implement a rigorous controlled vocabulary by:

- Creating variable names that adhere to conventions with [Jinja templating](#variable-creation-with-jinja-templating)
- Operating on subgroups of columns created by [custom macros](#variable-manipulation-with-regex-macros) to enforce contracts
- Validating subgroups of columns to ensure adherence to contracts with [custom tests](#data-validation-with-custom-tests)

## Scenario: COVID Forecast Model Monitoring

The full example code for this project is available [on GitHub](http://github.com/emilyriederer/dbt-convo-covid).

To illustrate these concepts, imagine we are tasked with monitoring the performance of a county-level COVID forecasting model using data similar to datasets available through [Google BigQuery public dataset program](https://cloud.google.com/blog/products/data-analytics/publicly-available-covid-19-data-for-analytics). We might want to continually log forecasted versus actual observations to ask questions like:

- Does the forecast perform well?
- How far in advance does the forecast become reliable?
- How does performance vary across counties?
- Is the performance acceptable in particularly sensitive counties, such as those with known health professional shortages?

Before we go further, a few caveats:

- I am not a COVID expert nor do I pretend to be. This is not a post about how one should monitor a COVID model. This is just an understandable, hypothetical example with data in a publicly available database^[In fact, many COVID models were unduly criticized because their purpose was not strictly to have the most accurate forecast possible.] 
- I do not attempt to demonstrate the best way to evaluate a forecasting model or a holistic approach to model monitoring. Again, this is just a hypothetical motivation to illustrate *data management* techniques
- This may seem like significant over-engineering for the problem at hand. Once again, this is just an example

Now, back to work.

### Controlled Vocabulary

To operationalize this analytical goal, we might start out by defining our controlled vocabulary with relevant concepts and contracts.

**Units of measurement**:

- `ID`: Unique identifier of entity with no other semantic meaning
  + Non-null
- `N`: Count
  + Integer
  + Non-null
- `DT`: Date
  + Date format
- `IND`: Binary indicator
  + Values of 0 or 1
  + Non-null
- `PROP`: Proportion
  + Numeric
  + Bounded between 0 and 1
- `PCT`: Percent
  + Numeric
  + Unlike `PROP`, *not* bounded (e.g. think "percent error")
- `CD`: System-generated character
  + Non-null
- `NM`: Human-readable name

**Units of observation**:

- `COUNTY`: US County
- `STATE`: US State
- `CASE`: Realized case (in practice, we would give this a more specific definition. What defines a case? What sort of confirmation is required? Is the event recorded on the date or realization or the date of reporting?)
- `HOSP`: Realized hospitalization (same note as above)
- `DEATH`: Realized death (same note as above)

**Descriptors**:

- `ACTL`: Actual observed value
- `PRED`: Predicted value
- `HPSA`: Health Professional Shortage Area (county-level measure)

### Data Sources and Flow

Our goal is to end up with a `model_monitor` table with one record per `observation date` and `county` (same as the `actual` table). Using the grammar above, we may define the variables we intend to include in our final table:

- `CD_(COUNTY|STATE)`: Unique county/state identifier (from Census Bureau FIPS codes)
- `NM_(COUNTY|STATE)`: Human-readable county/state names- 
- `DT_COUNTY`: The date a county's values are observed
- `N_(CASE|HOSP|DEATH)_(ACTL|PRED)_(07|14|21|28)`: The actual or predicted number of cases, hospitalizations, or deaths (and, for predictions only, the value of these predictions at 7, 14, 21, and 28 days prior to the day being forecasted)
- `IND_COUNTY_HPSA`: Indicator of whether county is considered a shortage area
- `PROP_COUNTY_HPSA`: Proportion of population that is underserved in a designated shortage area 

We will source these fields from four tables:

- `actual` table 
  + sourced from `bigquery-public-data`.`covid19_jhu_csse`.`summary`
  + one record per `observation date` x `county`
  + fields for county code, observation date, realized number of cases and deaths
- `prediction` table
  + sourced from `bigquery-public-data`.`covid19_public_forecasts`.`county_28d_historical`
  + one record per `date prediction was made` x `data being predicted` x `county` (initially)
  + fields for county code, observation date, prediction date, predicted number of cases and deaths
  + we transform to one record per `observation date` x `county` with observations at different time lags represented as separate fields
- `hpsa` table
  + sourced from `bigquery-public-data`.`sdoh_hrsa_shortage_areas`.`hpsa_primary_care`
  + (after some wrangling on our end) one record per `county` for counties identified as having a shortage
  + fields for the county code, date of designation, proportion of county under-served
- `fips` table
  + sourced from `bigquery-public-data`.`census_utility`.`fips_codes_all`
  + (after some wrangling) one record per `county` for each county in the 50 US states
  + fields for FIPS code (Census Bureau county identifiers), state name, county name

For a conceptual mental map, once all the wrangling and cleaning is done for each of the tables above, we might have psuedocode for the final table that looks something like this.

```
select *
from 
  actual 
    left join
  predictions using (cd_county, dt_county)
    left join
  hpsa using (cd_county)
    left join
  fips using (cd_county)
```

But as we're about to see, `dbt` allows us to get a bit more complex and elegant.

## Variable Creation with Jinja Templating

`dbt` makes it easy to create typo-free variable names that adhere to our controlled vocabulary by using the Jinja templating language.^[For another exploration of using Jinja templating to generate SQL, check out this nice [blog post](https://multithreaded.stitchfix.com/blog/2017/07/06/one-weird-trick/) from Stitch Fix] Jinja brings traditional control-flow elements like conditional statements and loops to make SQL more programmatic. When `dbt` is executed with `dbt run`, it first renders this Jinja to standard SQL before sending the query to the database. 

Templates, and specifically loops, help write more concise and proof-readable SQL code when deriving a large number of variables with similar logic. For example, below we collapse the raw prediction data (which is represented as one record for `each county` x `each day being prediction` x `each day a prediction was made`) to one record for each county and each day being predicted with different columns containing the numeric value of each prediction of cases, hospitalizations, and deaths at `lags` (defined in the `dbt_project.yml` configuration file) of 7, 14, 21, and 28 days prior to the date being predicted. 

Ordinarily, deriving these 12 variables (3 measures x 4 lags) would pose significant room for typos in either the code or the variable names, but in this script, the Jinja template of `n_case_pred_{{l}}` ensures consistency.

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/models/prediction.sql'), eval = FALSE}
```

This script renders to the following:

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/target/compiled/covid_model_monitor/models/prediction.sql'), eval = FALSE}
```

This script and the other three that derive our base tables (`actual`, `prediction`, `fips`, and `hpsa`) can be found in [the `models` directory](https://github.com/emilyriederer/dbt-convo-covid/tree/main/models) of the repo. After they are individually created, they are combined into the `model_monitor_staging` table in the relatively uninteresting [script](https://github.com/emilyriederer/dbt-convo-covid/blob/main/models/model_monitor_staging.sql):

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/models/model_monitor_staging.sql'), eval = FALSE}
```


## Variable Manipulation with Regex Macros

Of course, it's not enough to adhere to controlled vocabulary *naming*. If the actual *contracts* implied in those names are not upheld, the process is meaningless (or, worse, dangerous). When preparing our final table, we want to explicitly enforce as many of the vocabulary's promises to be met as possible. This means, for example, ensuring all variables prefixed with `n` are really integers, `dt` are truly dates (and not just similarly formatted strings), and `ind` variables are actually never-null.

This time, we again use Jinja templating along with another dbt feature: custom macros. The final script in our pipeline ([`model_monitor`](https://github.com/emilyriederer/dbt-convo-covid/blob/main/models/model_monitor.sql)) uses custom macros `get_column_names()` to determine all of the column names in the staging table and `get_matches()` to subset this list for variable names which match regular expressions corresponding to different prefixes. 

Then, we iterate over each of these lists to apply certain treatments to each set of columns such as casting `cols_n` and `cols_dt` variables to `int64` and `date` respectively, rounding `cols_prop` variables to three decimal places, and coalescing `cols_ind` variables to be 0 if null.^[Ordinarily, we would want to be careful setting null values to 0. We would not want to lie and imply the existence of missing data to nominally uphold a contract. However, this is the correct approach here. Our indicator variables in this case come from tables which only contain the `1` or "presence" values (e.g. the `hpsa` relation which provides `ind_county_hpsa` only has records for counties which are shortage areas) so this is a safe approach.] 

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/models/model_monitor.sql'), eval = FALSE}
```

Note how abstract this query template is. In fact, it completely avoids referencing specific variables in our table.^[In fact, this could also be a macro, as I introduce before, and shipped in a package to apply across all data models in an analytical database. To make the narrative of this example easier to follow, I leave it as a standard query model.] If we should decide to go back and add more fields (for example, actual and predicted recoveries) into our upstream models, they will receive the correct post-processing and validation as long as they are named appropriately.

For a peak under the hood, here's how those two macros work.

First, `get_column_names()` simply queries the databases' built in [`INFORMATION_SCHEMA`](https://en.wikipedia.org/wiki/Information_schema)^[An automatically created table containing metadata such as field names and types for each table in a database] to collect all column names of a given table. In the case of the `model_monitor.sql` script, the table provided is the staging table (`model_monitor_staging`) which was made in the previous step.

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/macros/get_column_names.sql'), eval = FALSE}
```

Next, the `get_matches()` macro simply iterates through a list of characters (such as the column names obtained in the previous step) and appends only those that match our regex to the final list that is returned.^[For those interested in the nitty gritty details, we must loop here because Jinja does not allow the more compact python list comprehensions. Additionally, Jinja only allows the python `append` method in display brackets `{{}}` so the `or ''` is a trick to silence the output, per [this site](http://svn.python.org/projects/external/Jinja-2.1.1/docs/_build/html/faq.html#isn-t-it-a-terrible-idea-to-put-logic-into-templates).] ^[Note that if you have installed dbt previously, this solution might not work for you. The python `re` library for regular expressions was not enabled inside dbt's Jinja until the recent release of [v0.19.0](https://github.com/fishtown-analytics/dbt/releases/tag/v0.19.0) ] (Thanks to [David Sanchez](https://twitter.com/dsmd4vid) on the `dbt` Slack community for helping me figure out how to call the `re` library from within Jinja.)

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/macros/get_matches.sql'), eval = FALSE}
```

These macros live in the [`macros/` directory](https://github.com/emilyriederer/dbt-convo-covid/tree/main/macros) of the repository.

## Data Validation with Custom Tests

Of course, not every contract can be made by force without risk of corrupting data. For any that we cannot enforce in their creation, we must rigorously test.

`dbt` allows simple, single-column tests such as `unique`, `not_null`, and `relationship` to be implemented in the `schema.yml` configuration file. This is useful, for example, for checking the validity of the keys (e.g. `cd_county`) that connect the tables. Tests specified under the `tests` key-value pair in the YAML for each relevant column, and can sometimes be shared across models with the YAML `&` and `*` which allows for naming and repeating blocks (think copy-paste).  However, even with a relatively small number of tests and columns, its cumbersome and easy to overlook a column.

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/models/schema.yml'), eval = FALSE}
```

Instead, developers may also define custom tests as SQL `select` statements which returns only records that fail the test. Like data models, tests may also use Jinja and macros. This allows us to abstract some of our data validation tests to target all variables with a specific naming convention (and, thus, performance contract) at any arbitrary point in the pipeline. 

For example, in the `model_monitor` data model shown in the last section, we explicitly cast all variables that start with `n` to be integers. However, before we do this, we should probably ensure that these fields are truly "integer-like"; otherwise, if we are casting values that have unexpected fractional components, we are simply masking inaccurate data.

The following test checks whether the `n` variables in the `model_monitor_staging` table (before casting) are sufficiently "integer like". It first retrieves all fields in this tables, next subsets all field names only to those with `n` prefixes, and finally uses Jinja to create a SQL script with separate `WHERE` conditions to check if the absolute difference between each `n` variable and its value after being cast to an integer is ever greater than 0.01 (which would imply a violation.)

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/tests/staging_n_int_like.sql'), eval = FALSE}
```

We can apply the same trick to testing more conditions on the final table. For example, the following test checks whether every `prop` variable is truly bounded between 0 and 1 (by returning any times where this is *not* the case.)

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/tests/final_prop_0_1.sql'), eval = FALSE}
```

Finally, we may also use tests to ensure our naming conventions are upheld. The following script once again calls the `INFORMATION_SCHEMA` table (as did our `get_column_names()` macro) to obtain a table with one record for each column name in the final table. It next uses the `regexp_extract()` SQL function with capturing groups to create separate columns (`l1`, `l2`, `l3`) for each underscore-delimited section of the naming. Finally, the `WHERE` conditions filter the output for any stubs that do not match the convention.

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/tests/con_vo.sql'), eval = FALSE}
```

As with our `model_monitor.sql` data model, the beauty of these tests is that they have abstracted away the column names themselves. So, they will continue to test all of the correct pieces of intent regardless of whether columns are added or removed from the table. Like macros, these could also be put into a package so that the same tests could be applied to all tables in a database. 

The code for these tests, and a few more similar examples, are located in the [`tests/` directory](https://github.com/emilyriederer/dbt-convo-covid/tree/main/tests) of the repository. They can be run on the command line with the `dbt test` command.

## Sample Output

To conclude, I show a few top rows of output from the final model monitoring table:

```{r eval = FALSE}
select * 
from dbt_emily.model_monitor
limit 5
```

```{r echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}
library(DBI)
library(odbc)
library(dplyr)

con <- dbConnect(odbc(),
                 Driver         = "Simba ODBC Driver for Google BigQuery",
                 Catalog        = "sonorous-wharf-302611",
                 Email          = "emilyriederer@gmail.com",
                 KeyFilePath    = "/Users/emily/Downloads/sonorous-wharf-302611-20015e7ecbcc.json",
                 OAuthMechanism = 0)

dbGetQuery(con, "select * from dbt_emily.model_monitor limit 5") %>%
  select(cd_county, dt_county, everything()) %>%
  knitr::kable()
```

## Bonus - Analysis Prep with Jinja Templates

Although this post primarily focuses on uses of `dbt` to help data producers apply controlled vocabularies, dbt also provides an interesting framework for transitioning projects to data consumers with the use of their [Analyses](https://docs.getdbt.com/docs/building-a-dbt-project/analyses) feature. Analyses are additional SQL script templates that are not sent to the database to produce tables or views.Instead, running `dbt compile` simply renders these scripts for use in analyses or BI tools. 

For example of an "analysis", and as another example of templating in action, the following script uses our published table to compute the percent difference between actual observations and each prediction.

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/analysis/model_monitor_metrics.sql'), eval = FALSE}
```

It compiles to:

```{r code=xfun::read_utf8('../../../../Documents/dbt-covid/target/compiled/covid_model_monitor/analysis/model_monitor_metrics.sql'), eval = FALSE}
```

