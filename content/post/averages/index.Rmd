---
output: hugodown::md_document
title: "Draft"
subtitle: ""
summary: ""
authors: []
tags: []
categories: [data]
date: 2021-09-27
lastmod: 2021-09-27
featured: false
draft: false
aliases:

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: true

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: [""]
---

Last month, I dusted off a training course that I first developed in 2018 to teach to a class of new hires at work. The content focuses on many types of *data disasters*^[Similar to the long-form writing project I'm working in the open over [here](https://data-disasters.netlify.app/)] that are often encountered when working with data -- from misunderstanding how data is structured and tools work to errors in causal inference and model building -- yet rarely taught in stats training. 

This year, however, the material felt a bit different, and it was not simply because (for now the second year) I was sitting alone in a room all afternoon rambling to myself and a Zoom screen about all the ways everything can go wrong.

{{< tweet 1305666403824021507 >}}

This year, the material took on a whole new level of gravity. In a nutshell, as I walked through my taxonomy and examples of data disasters, one of my underlying message had always been that stats education focuses on the "complex" things (e.g. proving asymptotic convergence) but simple things can also be hard and important. However, as the pandemic has evolved over the last year and a half, more highly important data analysis has been done in the open than ever before, and much of it has unsurprisingly hit many of the snags one would when working with messy, inconsistent, ever-changing, observational data. 

As I walked through the course, I found myself thinking about many pandemic-related examples for every "disasters" which all felt more urgent and timely than the industry-specific examples that I had concocted. In this post, I walk through just one such example of simple things that are hard. This is the story of a [simple average informing public health policy in Indiana](https://www.indystar.com/story/news/health/2020/12/23/covid-indiana-positivity-rate-error-corrected-dec-30/4013741001/) and what it illustrates about imprecise estimands, (possibly BI tools), independence assumptions, de Moivre's equation, and sampling bias.

## What happened?

> "The change to the methodology is how we calculate the seven-day positivity rate for counties. In the past, similar to many states, we've added each day's positivity rate for seven days and divided by seven to obtain the week's positivity rate. Now we will add all of the positive tests for the week and divide by the total tests done that week to determine the week's positivity rate. This will help to minimize the effect that a high variability in the number of tests done each day can have on the week's overall positivity, especially for our smaller counties."


## What went wrong?

### What is the target?

The first step to tracking a metric is defining the metric to track. This sounds obvious, but too often it is not. Metric design requires a lot of thought^[One framework for approaching metric design is eloquently explored by Sean Taylor in this essay: https://medium.com/@seanjtaylor/designing-and-evaluating-metrics-5902ad6873bf] yet often glossed over even in important contexts such as medical research^[This new study cites many examples of poorly defined estimands (think metrics) from the BMJ: https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-021-05644-4]. In more complex problems, this will affect the research design and statistical methods we use to derive results, but in simpler cases like this, it can simply alter basic arithmetic. 

So, the problem here starts with the very definition of what we are trying to track:

> average proportion of positive test results over the past seven days

Now, that seems pretty specific. An average of a proportion with a clear numerator and denominator over a defined time horizon. Easy, right? Not so fast. There's more nuance than meets the eye in how we calculate *averages* and what technically "happened" in the *last seven days*.

### Average over what?

At some point in elementary school, we were probably all taught that you calculate an average by adding up a bunch of numbers and dividing the total by the number of numbers we started with. That's not *wrong*, but it's actually more of a special case than a general rule. 

Later on, we might have learned about the *weighted average* as a separate concept. There, instead of `sum(x) / count(x)`, we might have learned another formula like `sum( x * weight(x) ) / sum(weight(x))`. Of course, this isn't so much an extension as an abstraction (where normal arithmetic averages are recreated when the weight is set to 1 for all values of the number in question.)

So, ultimately, all averages are weighted averages whether we acknowledge it or not. That, in turn, suggests that whenever we define a metric as an *average*, the obligatory next question is "the average over what?"

What I suspect happened in the case from Indiana, is that they were taking unweighted arithmetic average of the test positivity proportions for the last seven days. That is, they were using the *average of days* instead of the *average over tests*. 

To see the difference, suppose there are 50 tests on day 1 with 20% positive and 100 tests on day 2 with 10% positivity. Averaging over days gives up a value of 15% and over tests a value of 13.3%

```{r results = "hold"}
(avg_of_days <- (10/50 + 10/100) / 2)
(avg_of_test <- (10 + 10) / (50 + 100))
```

It's worth nothing that what's happening here is also mathematically equivallent to comparing the average of the proportions instead of the proportions of sums.

```{r results = "hold"}
(avg_of_props <- (10/50 + 10/100) / 2)
(props_of_sums <- (10 + 10) / (50 + 100))
```

And, alternatively, none of this matters if the number of tests are the same on both days because then our weights are constant and we are back to the flat arithmetic average over observations (days).

```{r results = "hold"}
(avg_of_props <- (20/100 + 10/100) / 2)
(props_of_sums <- (20 + 10) / (100 + 100))
```

### Does sample size cause problems?

So, we probably used the wrong type of average for the question at hand, but we probably could have gotten away with it if the sample sizes across days were not substantially different. We say how the sample size affects the weighting, but can it cause other statistical problems? 

Suppose for a minute, that this is a binomial setup where each test has an equal probability of coming back positive (it does not). We can simulate a number of draws from binomial distributions of different sample sizes, compute the sample proportion of successes, and inspect the mean and variance. Below, we confirm what we would expect that small samples produce unbiased but high variance estimates^[We also know this with standard statistical results and formulas, but it's more fun to see it.].

```{r results = "hold"}
p <- 0.5
n <- 1000
size <- c(10, 100, 500)
samples <- lapply(size, FUN = function(x) rbinom(n, x, p) / x)
vapply(samples, FUN = function(x) round(mean(x), 3), FUN.VALUE = numeric(1))
vapply(samples, FUN = function(x) round(sd(x), 3), FUN.VALUE = numeric(1))
```

If staring at summary statistics is uninspiring, we can wrap our minds around the extend of the difference with a plot.

```{r echo = FALSE}
library(ggplot2)

df <- data.frame(
  s = unlist(lapply(size, FUN = function(x) rep(x,n))), 
  x = unlist(samples))

ggplot(data = df) +
  aes(x = x, col = as.character(s)) +
  geom_density() +
  geom_vline(xintercept = p, col = 'darkgrey', linetype = 2) +
  labs(
    title = "Sampling Distribution for p = 0.5",
    col = "Sample Size"
  ) +
  scale_x_continuous(breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"
  )
```

So, days with smaller samples are most likely to take the most extreme values which could yank an average in one direction *or the other* ephemerally^[As discussed at length in this Scientific American article: https://www.americanscientist.org/article/the-most-dangerous-equation]. This alone is a bad situation for a metric of interest; however, it can't be the full story. The initial article implies that the positivity rate was being *consistently overestimated*, that is, biased.

### Is sample size suggestive of a problem?

The vagaries of sample size may not independently be causing this problem, but they do raise a few broader issues. First, unweighted arithmetic averages make the most sense when we can make statistics' favorite assumptions of "independent and identically distributed". If, in fact, we see large variances in sample size, that would suggest this is not the case. 





```{r results = "hold"}
avg_of_ratios <- (10/100 + 90/100) / 2

ratio_of_sums <- (10 + 90) / (100 + 100)

avg_of_ratios == ratio_of_sums

avg_of_ratios_uneq <- (10/100 + 180 / 200) / 2

ratio_of_sums_uneq <- (10 + 180) / (100 + 200)

avg_of_ratios_uneq == ratio_of_sums_uneq

weightavg_of_ratios_uneq <- (100/300)*(10/100) + (200/300)*(180/200)

ratio_of_sums_uneq == weightavg_of_ratios_uneq
```

```{r results = "hold"}
df1 <- data.frame(
  numer = c( 10,  90),
  denom = c(100, 100)
)

with(df1, mean(numer)/mean(denom))
with(df1, mean(numer / denom))
```

```{r results = "hold"}
df2 <- data.frame(
  numer = c( 10,  90*2),
  denom = c(100, 100*2)
)

with(df2, mean(numer)/mean(denom))
with(df2, mean(numer / denom))
```


## A sidenote on BI tools

## Independence assumptions

defining estimand forces us to think about what are we even doing here? 

average can be for denoising or smoothing

implicit assumption of iid if using for denoising; otherwise it's not just noise you're canceling out

other time, you may not care if you are just going to treat everything the same anyway

## Sample Size 

if data is from same distribution, this could increase variance but shouldn't effect mean

Recall that the standard deviation of sample proportion is $\sqrt(p*(1-p)/n)$

link to discussions of sample size and different types of averages

```{r}
set.seed(123)

# define simulation parameters ----
## n: total draws from binomial distribution
## p: proportion of successes
p <- 0.5
n <- 1000

# sample from binomials of different size ----
s010 <- rbinom(n,  10, p) /  10
s100 <- rbinom(n, 100, p) / 100
s500 <- rbinom(n, 500, p) / 500

# set results as dataframe for inspection ----
df <- data.frame(
  s = rep(c(10, 100, 500), each = n),
  x = c(s010, s100, s500)
)
```


```{r}
p <- 0.5
n <- 1000
size <- c(10, 100, 500)
samples <- lapply(size, FUN = function(x) rbinom(n, x, p) / x)
vapply(samples, function(x) round(mean(x), 3), numeric(1))
vapply(samples, function(x) round(sd(x), 3), numeric(1))
```


```{r}
aggregate(df$x, by = list(df$s), FUN = mean)
aggregate(df$x, by = list(df$s), FUN = sd)
```


```{r}
library(ggplot2)

ggplot(data = df) +
  aes(x = x, col = as.character(s)) +
  geom_density() +
  geom_vline(xintercept = p, col = 'darkgrey', linetype = 2) +
  labs(
    title = "Sampling Distribution for p = 0.5",
    col = "Sample Size"
  ) +
  scale_x_continuous(breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"
  )
```

```{r}
p <- 0.5
n <- c(rep(100, 2), rep(1000, 7))
est <- lapply(n, FUN = function(x) rbinom(100, size = x, prob = p) / x)
```


## Bias

## Conclusion





then back to the data for why it matters. 



but low sample days based on real world are probably also a sign of a different distribution (only very urgent cases get tested?) 





